{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master DSLS / Programming 3 / Assignment 6\n",
    "# Final Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "https://bioinf.nl/~martijn/master/programming3/assignment6.html\n",
    "\n",
    "This is the final for programming 3. In this assignment, I will develop scikit-learn machine learning models to predict the function of the proteins in the specific dataset. This model will use small InterPro_annotations_accession to predict large InterPro_annotations_accession.\n",
    "The definition of small InterPro_annotations_accession and large InterPro_annotations_accession is defined as below:\n",
    "\n",
    "If InterPro_annotations_accession's feature length(Stop_location-Start_location) / Sequence_length > 0.9, it is large InterPro_annotations_accession.\n",
    "\n",
    "Otherwise, it is a small InterPro_annotations_accession.\n",
    "\n",
    "We can briefly rewrite as:\n",
    "\n",
    "            |(Stop - Start)|/Sequence >  0.9 --> Large\n",
    "\n",
    "            |(Stop - Start)|/Sequence <= 0.9 --> small\n",
    "\n",
    "I will also check the \"bias\" and \"noise\" that does not make sense from the dataset.\n",
    "\n",
    "ie. lines(-) from the TSV file which don't contain InterPRO numbers\n",
    "\n",
    "ie. proteins which don't have a large feature (according to the criteria above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Goal\n",
    "\n",
    "The goal of this assignment is to predict large InterPro_annotations_accession by small InterPro_annotations_accession.\n",
    "\n",
    "I will use the dataset from /data/dataprocessing/interproscan/all_bacilli.tsv file on assemblix2012 and assemblix2019. However, this file contains ~4,200,000 protein annotations, so I will put a subset of all_bacilli.tsv on GitHub and on local for code testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bassemblix2019/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/__init__.py:51\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconf\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkConf\n\u001b[0;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontext\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[1;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrdd\u001b[39;00m \u001b[39mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[1;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfiles\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkFiles\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/context.py:31\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtempfile\u001b[39;00m \u001b[39mimport\u001b[39;00m NamedTemporaryFile\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpy4j\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotocol\u001b[39;00m \u001b[39mimport\u001b[39;00m Py4JError\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m accumulators\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maccumulators\u001b[39;00m \u001b[39mimport\u001b[39;00m Accumulator\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbroadcast\u001b[39;00m \u001b[39mimport\u001b[39;00m Broadcast, BroadcastPickleRegistry\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/accumulators.py:97\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msocketserver\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mSocketServer\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreading\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserializers\u001b[39;00m \u001b[39mimport\u001b[39;00m read_int, PickleSerializer\n\u001b[1;32m    100\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAccumulator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAccumulatorParam\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    103\u001b[0m pickleSer \u001b[39m=\u001b[39m PickleSerializer()\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/serializers.py:71\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     protocol \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     69\u001b[0m     xrange \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m cloudpickle\n\u001b[1;32m     72\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m _exception_message\n\u001b[1;32m     75\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mPickleSerializer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMarshalSerializer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mUTF8Deserializer\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/cloudpickle.py:145\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39mCodeType(\n\u001b[1;32m    127\u001b[0m             co\u001b[39m.\u001b[39mco_argcount,\n\u001b[1;32m    128\u001b[0m             co\u001b[39m.\u001b[39mco_kwonlyargcount,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m             (),\n\u001b[1;32m    142\u001b[0m         )\n\u001b[0;32m--> 145\u001b[0m _cell_set_template_code \u001b[39m=\u001b[39m _make_cell_set_template_code()\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcell_set\u001b[39m(cell, value):\n\u001b[1;32m    149\u001b[0m     \u001b[39m\"\"\"Set the value of a closure cell.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/cloudpickle.py:126\u001b[0m, in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39mCodeType(\n\u001b[1;32m    110\u001b[0m         co\u001b[39m.\u001b[39mco_argcount,\n\u001b[1;32m    111\u001b[0m         co\u001b[39m.\u001b[39mco_nlocals,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         (),\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39;49mCodeType(\n\u001b[1;32m    127\u001b[0m         co\u001b[39m.\u001b[39;49mco_argcount,\n\u001b[1;32m    128\u001b[0m         co\u001b[39m.\u001b[39;49mco_kwonlyargcount,\n\u001b[1;32m    129\u001b[0m         co\u001b[39m.\u001b[39;49mco_nlocals,\n\u001b[1;32m    130\u001b[0m         co\u001b[39m.\u001b[39;49mco_stacksize,\n\u001b[1;32m    131\u001b[0m         co\u001b[39m.\u001b[39;49mco_flags,\n\u001b[1;32m    132\u001b[0m         co\u001b[39m.\u001b[39;49mco_code,\n\u001b[1;32m    133\u001b[0m         co\u001b[39m.\u001b[39;49mco_consts,\n\u001b[1;32m    134\u001b[0m         co\u001b[39m.\u001b[39;49mco_names,\n\u001b[1;32m    135\u001b[0m         co\u001b[39m.\u001b[39;49mco_varnames,\n\u001b[1;32m    136\u001b[0m         co\u001b[39m.\u001b[39;49mco_filename,\n\u001b[1;32m    137\u001b[0m         co\u001b[39m.\u001b[39;49mco_name,\n\u001b[1;32m    138\u001b[0m         co\u001b[39m.\u001b[39;49mco_firstlineno,\n\u001b[1;32m    139\u001b[0m         co\u001b[39m.\u001b[39;49mco_lnotab,\n\u001b[1;32m    140\u001b[0m         co\u001b[39m.\u001b[39;49mco_cellvars,  \u001b[39m# this is the trickery\u001b[39;49;00m\n\u001b[1;32m    141\u001b[0m         (),\n\u001b[1;32m    142\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type bytes)"
     ]
    }
   ],
   "source": [
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type bytes)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix2019/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Spark web UI:http://localhost:4040/jobs/\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix2019/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Output format : https://interproscan-docs.readthedocs.io/en/latest/OutputFormats.html\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bassemblix2019/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix2019/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m StructType, StructField, StringType, FloatType, IntegerType\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/__init__.py:51\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconf\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkConf\n\u001b[0;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontext\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[1;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrdd\u001b[39;00m \u001b[39mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[1;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfiles\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkFiles\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/context.py:31\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtempfile\u001b[39;00m \u001b[39mimport\u001b[39;00m NamedTemporaryFile\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpy4j\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotocol\u001b[39;00m \u001b[39mimport\u001b[39;00m Py4JError\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m accumulators\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maccumulators\u001b[39;00m \u001b[39mimport\u001b[39;00m Accumulator\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbroadcast\u001b[39;00m \u001b[39mimport\u001b[39;00m Broadcast, BroadcastPickleRegistry\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/accumulators.py:97\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msocketserver\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mSocketServer\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreading\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserializers\u001b[39;00m \u001b[39mimport\u001b[39;00m read_int, PickleSerializer\n\u001b[1;32m    100\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAccumulator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAccumulatorParam\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    103\u001b[0m pickleSer \u001b[39m=\u001b[39m PickleSerializer()\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/serializers.py:71\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     protocol \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     69\u001b[0m     xrange \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m\n\u001b[0;32m---> 71\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m cloudpickle\n\u001b[1;32m     72\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m _exception_message\n\u001b[1;32m     75\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mPickleSerializer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMarshalSerializer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mUTF8Deserializer\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/cloudpickle.py:145\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39mCodeType(\n\u001b[1;32m    127\u001b[0m             co\u001b[39m.\u001b[39mco_argcount,\n\u001b[1;32m    128\u001b[0m             co\u001b[39m.\u001b[39mco_kwonlyargcount,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m             (),\n\u001b[1;32m    142\u001b[0m         )\n\u001b[0;32m--> 145\u001b[0m _cell_set_template_code \u001b[39m=\u001b[39m _make_cell_set_template_code()\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcell_set\u001b[39m(cell, value):\n\u001b[1;32m    149\u001b[0m     \u001b[39m\"\"\"Set the value of a closure cell.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/commons/conda/lib/python3.9/site-packages/pyspark/cloudpickle.py:126\u001b[0m, in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39mCodeType(\n\u001b[1;32m    110\u001b[0m         co\u001b[39m.\u001b[39mco_argcount,\n\u001b[1;32m    111\u001b[0m         co\u001b[39m.\u001b[39mco_nlocals,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         (),\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39;49mCodeType(\n\u001b[1;32m    127\u001b[0m         co\u001b[39m.\u001b[39;49mco_argcount,\n\u001b[1;32m    128\u001b[0m         co\u001b[39m.\u001b[39;49mco_kwonlyargcount,\n\u001b[1;32m    129\u001b[0m         co\u001b[39m.\u001b[39;49mco_nlocals,\n\u001b[1;32m    130\u001b[0m         co\u001b[39m.\u001b[39;49mco_stacksize,\n\u001b[1;32m    131\u001b[0m         co\u001b[39m.\u001b[39;49mco_flags,\n\u001b[1;32m    132\u001b[0m         co\u001b[39m.\u001b[39;49mco_code,\n\u001b[1;32m    133\u001b[0m         co\u001b[39m.\u001b[39;49mco_consts,\n\u001b[1;32m    134\u001b[0m         co\u001b[39m.\u001b[39;49mco_names,\n\u001b[1;32m    135\u001b[0m         co\u001b[39m.\u001b[39;49mco_varnames,\n\u001b[1;32m    136\u001b[0m         co\u001b[39m.\u001b[39;49mco_filename,\n\u001b[1;32m    137\u001b[0m         co\u001b[39m.\u001b[39;49mco_name,\n\u001b[1;32m    138\u001b[0m         co\u001b[39m.\u001b[39;49mco_firstlineno,\n\u001b[1;32m    139\u001b[0m         co\u001b[39m.\u001b[39;49mco_lnotab,\n\u001b[1;32m    140\u001b[0m         co\u001b[39m.\u001b[39;49mco_cellvars,  \u001b[39m# this is the trickery\u001b[39;49;00m\n\u001b[1;32m    141\u001b[0m         (),\n\u001b[1;32m    142\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type bytes)"
     ]
    }
   ],
   "source": [
    "# Spark web UI:http://localhost:4040/jobs/\n",
    "# Output format : https://interproscan-docs.readthedocs.io/en/latest/OutputFormats.html\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "from pyspark.ml.feature import StringIndexer,VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df by PySpark\n",
    "start = time.time()\n",
    "schema = StructType([\n",
    "    StructField(\"Protein_accession\", StringType(), True),\n",
    "    StructField(\"Sequence_MD5_digest\", StringType(), True),\n",
    "    StructField(\"Sequence_length\", IntegerType(), True),\n",
    "    StructField(\"Analysis\", StringType(), True),\n",
    "    StructField(\"Signature_accession\", StringType(), True),\n",
    "    StructField(\"Signature_description\", StringType(), True),\n",
    "    StructField(\"Start_location\", IntegerType(), True),\n",
    "    StructField(\"Stop_location\", IntegerType(), True),\n",
    "    StructField(\"Score\", FloatType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"InterPro_annotations_accession\", StringType(), True),\n",
    "    StructField(\"InterPro_annotations_description\", StringType(), True),\n",
    "    StructField(\"GO_annotations\", StringType(), True),\n",
    "    StructField(\"Pathways_annotations\", StringType(), True)])\n",
    "path = \"/data/dataprocessing/interproscan/all_bacilli.tsv\"\n",
    "spark = SparkSession.builder.master(\"local[16]\")\\\n",
    "                            .config('spark.driver.memory', '128g')\\\n",
    "                            .config('spark.executor.memory', '128g')\\\n",
    "                            .config(\"spark.sql.debug.maxToStringFields\",\"100\")\\\n",
    "                            .appName(\"InterPro\").getOrCreate()\n",
    "df = spark.read.option(\"sep\",\"\\t\").option(\"header\",\"False\").csv(path,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove InterPro_annotations_accession == \"-\"\n",
    "# get the length of protein\n",
    "# get the ratio to distinguish them to large and small InterPro_annotations_accession\n",
    "# 1 for large, 0 for small InterPro_annotations_accession\n",
    "df = df.filter(df.InterPro_annotations_accession != \"-\")\\\n",
    "        .withColumn(\"Ratio\", (abs(df[\"Stop_location\"] - df[\"Start_location\"])/df[\"Sequence_length\"]))\\\n",
    "        .withColumn(\"Size\", when((abs(df[\"Stop_location\"] - df[\"Start_location\"])/df[\"Sequence_length\"])>0.9,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intersection to make sure there is a match of large and small InterPro_annotations_accession(at least one large and one small InterPro_annotations_accession)\n",
    "intersection = df.filter(df.Size == 0).select(\"Protein_accession\").intersect(df.filter(df.Size == 1).select(\"Protein_accession\"))\n",
    "intersection_df = intersection.join(df,[\"Protein_accession\"])\n",
    "\n",
    "# get the number of small InterPro_annotations_accession in each Protein_accession\n",
    "small_df = intersection_df.filter(df.Size == 0).groupBy([\"Protein_accession\"]).pivot(\"InterPro_annotations_accession\").count()\n",
    "\n",
    "# There are several InterPro_annotations_accession with the same Protein_accession. I only choose the largest one.\n",
    "large_df = intersection_df.filter(df.Size == 1).groupby([\"Protein_accession\"]).agg(max(\"Ratio\").alias(\"Ratio\"))\n",
    "large_df = large_df.join(intersection_df,[\"Protein_accession\",\"Ratio\"],\"inner\").dropDuplicates([\"Protein_accession\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the df for ML, we do not need Protein_accession anymore.\n",
    "ML_df = large_df.join(small_df,[\"Protein_accession\"],\"outer\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "# plot the small InterPro_annotations_accession in each Protein_accession\n",
    "MLdf = ML_df.toPandas()\n",
    "MLdf[\"protein_sum\"] = MLdf.iloc[:,17:].sum(axis=1)\n",
    "MLdf = MLdf.sort_values(\"Sequence_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(MLdf)),MLdf.protein_sum)\n",
    "plt.title(\"Protein sequence & small interpro\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Frequency of small InterPro annotations accession\")\n",
    "plt.savefig(\"Protein sequence & small interpro.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the useless columns\n",
    "columns = (\"Sequence_MD5_digest\",\"Analysis\",\"Signature_accession\",\"Signature_description\",\n",
    "        \"Score\",\"Status\",\"Date\",\"InterPro_annotations_description\",\"GO_annotations\",\"Protein_accession\",\n",
    "        \"Pathways_annotations\",\"Ratio\",\"Size\",\"Stop_location\",\"Start_location\",\"Sequence_length\")\n",
    "ML_df = ML_df.drop(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catalogize y variable\n",
    "Label = StringIndexer(inputCol=\"InterPro_annotations_accession\", outputCol=\"InterPro_index\")\n",
    "\n",
    "# catalogize X variable\n",
    "input_columns = ML_df.columns[1:]\n",
    "assembler = VectorAssembler(inputCols=input_columns,outputCol=\"InterPro_features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[Label,assembler])\n",
    "ML_final = pipeline.fit(ML_df).transform(ML_df)\n",
    "\n",
    "# Setup X, y and split it\n",
    "(trainData, testData) = ML_final.randomSplit([0.7, 0.3],seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "# create a model\n",
    "rf = RandomForestClassifier(labelCol=\"InterPro_index\",\n",
    "                            featuresCol=\"InterPro_features\",\n",
    "                            predictionCol=\"prediction\", \n",
    "                            seed=42,maxDepth=20,\n",
    "                            maxMemoryInMB = 256,\n",
    "                            numTrees=500)\n",
    "rf_Model = rf.fit(trainData)\n",
    "predict = rf_Model.transform(testData)\n",
    "\n",
    "# evaluate the result\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='InterPro_index',\n",
    "                                            predictionCol = 'prediction',\n",
    "                                            metricName='accuracy')\n",
    "\n",
    "accuracy = evaluator.evaluate(predict)\n",
    "print(f\"Accuracy is {accuracy}\")\n",
    "# 0.3044988743110007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning RandomForestClassifier\n",
    "# create a model\n",
    "rf = RandomForestClassifier(labelCol=\"InterPro_index\",\n",
    "                            featuresCol=\"InterPro_features\",\n",
    "                            predictionCol=\"prediction\", \n",
    "                            seed=42,\n",
    "                            maxMemoryInMB = 256)\n",
    "\n",
    "# Tuning\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(rf.maxDepth, [5,10,20])\n",
    "            .addGrid(rf.numTrees, [20,100])\n",
    "            .build())\n",
    "\n",
    "# evaluate the result\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol='InterPro_index',\n",
    "                                            predictionCol = 'prediction',\n",
    "                                            metricName='accuracy')\n",
    "\n",
    "# KFold\n",
    "cv = CrossValidator(estimator=rf,\n",
    "                    evaluator=rf_evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=5,\n",
    "                    parallelism=10,\n",
    "                    seed=42)\n",
    "\n",
    "# Run Cross-validation\n",
    "rf_cvModel = cv.fit(trainData)\n",
    "\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "rf_cvPredictions = rf_cvModel.transform(testData)\n",
    "\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "rf_evaluator.evaluate(rf_cvPredictions)\n",
    "# 0.30174287710581477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision trees\n",
    "dtc = DecisionTreeClassifier(labelCol=\"InterPro_index\",\n",
    "                            featuresCol=\"InterPro_features\",\n",
    "                            predictionCol=\"prediction\")\n",
    "dtc = dtc.fit(trainData)\n",
    "dtc_pred = dtc.transform(testData)\n",
    "dtc_evaluator=MulticlassClassificationEvaluator(labelCol='InterPro_index',\n",
    "                                                predictionCol = 'prediction',\n",
    "                                                metricName='accuracy')\n",
    "dtc_acc = dtc_evaluator.evaluate(dtc_pred)\n",
    "print(\"Prediction Accuracy: \", dtc_acc)\n",
    "# 0.09174365344305567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning DecisionTree\n",
    "# Tuning\n",
    "dtc = DecisionTreeClassifier(labelCol=\"InterPro_index\",\n",
    "                            featuresCol=\"InterPro_features\",\n",
    "                            predictionCol=\"prediction\")  \n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(dtc.maxDepth, [2,4,6,8,10,12])\n",
    "            .build())\n",
    "\n",
    "# evaluate the result\n",
    "dtc_evaluator = MulticlassClassificationEvaluator(labelCol='InterPro_index',\n",
    "                                            predictionCol = 'prediction',\n",
    "                                            metricName='accuracy')\n",
    "\n",
    "# KFold\n",
    "cv = CrossValidator(estimator=dtc,\n",
    "                    evaluator=dtc_evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=5,\n",
    "                    parallelism=10,\n",
    "                    seed=42)\n",
    "\n",
    "# Run Cross-validation\n",
    "dtc_cvModel = cv.fit(trainData)\n",
    "\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "dtc_cvPredictions = dtc_cvModel.transform(testData)\n",
    "\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "dtc_evaluator.evaluate(dtc_cvPredictions)\n",
    "# 0.13698470615635433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "nb = NaiveBayes(modelType=\"multinomial\",labelCol=\"InterPro_index\",\n",
    "                    featuresCol=\"InterPro_features\",\n",
    "                    predictionCol=\"prediction\",)    \n",
    "nb = nb.fit(trainData)\n",
    "nb_pred = nb.transform(testData)\n",
    "nb_evaluator=MulticlassClassificationEvaluator(labelCol='InterPro_index',\n",
    "                                                predictionCol = 'prediction',\n",
    "                                                metricName='accuracy')\n",
    "nb_acc = nb_evaluator.evaluate(nb_pred)\n",
    "print(\"Prediction Accuracy: \", nb_acc)\n",
    "# 0.7822956292213338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning Naive Bayes model\n",
    "# Tuning\n",
    "nb = NaiveBayes(modelType=\"multinomial\",labelCol=\"InterPro_index\",\n",
    "                    featuresCol=\"InterPro_features\",\n",
    "                    predictionCol=\"prediction\",)    \n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0])\n",
    "            .build())\n",
    "\n",
    "# evaluate the result\n",
    "nb_evaluator = MulticlassClassificationEvaluator(labelCol='InterPro_index',\n",
    "                                            predictionCol = 'prediction',\n",
    "                                            metricName='accuracy')\n",
    "\n",
    "# KFold\n",
    "cv = CrossValidator(estimator=nb,\n",
    "                    evaluator=nb_evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=5,\n",
    "                    parallelism=10,\n",
    "                    seed=42)\n",
    "\n",
    "# Run Cross-validation\n",
    "nb_cvModel = cv.fit(trainData)\n",
    "\n",
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "nb_cvPredictions = nb_cvModel.transform(testData)\n",
    "\n",
    "# Evaluate bestModel found from Cross Validation\n",
    "nb_evaluator.evaluate(nb_cvPredictions)\n",
    "# 0.8476049996118313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "trainFile = '/students/2021-2022/master/DaanSteur_DSLS/trainData.pkl'\n",
    "trainData.toPandas().set_index('InterPro_annotations_accession').to_pickle(trainFile)\n",
    "testFile = '/students/2021-2022/master/DaanSteur_DSLS/testData.pkl'\n",
    "testData.toPandas().set_index('InterPro_annotations_accession').to_pickle(testFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "nb_cvModel.bestModel.write().overwrite().save(\"/students/2021-2022/master/DaanSteur_DSLS/NaiveBayesBestModel\")\n",
    "dtc_cvModel.bestModel.write().overwrite().save(\"/students/2021-2022/master/DaanSteur_DSLS/DecisionTreeBestModel\")\n",
    "rf_cvModel.bestModel.write().overwrite().save(\"/students/2021-2022/master/DaanSteur_DSLS/RandomForestBestModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original script\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,FloatType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "# Create a df by PySpark\n",
    "start = time.time()\n",
    "schema = StructType([\n",
    "    StructField(\"Protein_accession\", StringType(), True),\n",
    "    StructField(\"Sequence_MD5_digest\", StringType(), True),\n",
    "    StructField(\"Sequence_length\", IntegerType(), True),\n",
    "    StructField(\"Analysis\", StringType(), True),\n",
    "    StructField(\"Signature_accession\", StringType(), True),\n",
    "    StructField(\"Signature_description\", StringType(), True),\n",
    "    StructField(\"Start_location\", IntegerType(), True),\n",
    "    StructField(\"Stop_location\", IntegerType(), True),\n",
    "    StructField(\"Score\", FloatType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"InterPro_annotations_accession\", StringType(), True),\n",
    "    StructField(\"InterPro_annotations_description\", StringType(), True),\n",
    "    StructField(\"GO_annotations\", StringType(), True),\n",
    "    StructField(\"Pathways_annotations\", StringType(), True)])\n",
    "path = \"/data/dataprocessing/interproscan/all_bacilli.tsv\"\n",
    "# path = \"all_bacilli.tsv\"\n",
    "spark = SparkSession.builder.master(\"local[16]\")\\\n",
    "                            .config('spark.driver.memory', '128g')\\\n",
    "                            .config('spark.executor.memory', '128g')\\\n",
    "                            .config(\"spark.sql.debug.maxToStringFields\",\"100\")\\\n",
    "                            .appName(\"InterPro\").getOrCreate()\n",
    "df = spark.read.option(\"sep\",\"\\t\").option(\"header\",\"False\").csv(path,schema=schema)\n",
    "df = df.filter(df.InterPro_annotations_accession != \"-\")\\\n",
    "        .withColumn(\"Length\", abs(df[\"Stop_location\"] - df[\"Start_location\"]))\\\n",
    "        .withColumn(\"Ratio\", (abs(df[\"Stop_location\"] - df[\"Start_location\"])/df[\"Sequence_length\"]))\\\n",
    "        .withColumn(\"Size\", when((abs(df[\"Stop_location\"] - df[\"Start_location\"])/df[\"Sequence_length\"])>0.9,1).otherwise(0))\n",
    "# get the intersection to make sure there is a match of large and small InterPro_annotations_accession(at least one large and one small InterPro_annotations_accession)\n",
    "intersection = df.filter(df.Size == 0).select(\"Protein_accession\").intersect(df.filter(df.Size == 1).select(\"Protein_accession\"))\n",
    "intersection_df = intersection.join(df,[\"Protein_accession\"])\n",
    "# get the number of small InterPro_annotations_accession in each Protein_accession\n",
    "small_df = intersection_df.filter(df.Size == 0).groupBy([\"Protein_accession\"]).pivot(\"InterPro_annotations_accession\").count()\n",
    "# There are several InterPro_annotations_accession with the same Protein_accession. I only choose the largest one.\n",
    "large_df = intersection_df.filter(df.Size == 1).groupby([\"Protein_accession\"]).agg(max(\"Ratio\").alias(\"Ratio\"))\n",
    "large_df = large_df.join(intersection_df,[\"Protein_accession\",\"Ratio\"],\"inner\").dropDuplicates([\"Protein_accession\"])\n",
    "# Drop the useless columns\n",
    "columns = (\"Sequence_MD5_digest\",\"Analysis\",\"Signature_accession\",\"Signature_description\",\n",
    "        \"Score\",\"Status\",\"Date\",\"InterPro_annotations_description\",\"GO_annotations\",\n",
    "        \"Pathways_annotations\",\"Ratio\",\"Size\",\"Stop_location\",\"Start_location\",\"Sequence_length\",\"Length\")\n",
    "large_df = large_df.drop(*columns)\n",
    "# Create the df for ML\n",
    "ML_df = large_df.join(small_df,[\"Protein_accession\"]).fillna(0)\n",
    "# Setup X,y and split it\n",
    "y = ML_df.select(\"InterPro_annotations_accession\")\n",
    "X = ML_df.select(ML_df.columns[2:])\n",
    "y = np.array(y.collect())\n",
    "X = np.array(X.collect())\n",
    "\n",
    "# ordinal encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "y = label_encoder.transform(y)\n",
    "sk = StratifiedKFold(n_splits=2, shuffle=True,random_state=42)\n",
    "for train_index, test_index in sk.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "# Try standard#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Modeling\n",
    "rfc = RandomForestClassifier(random_state=42,max_depth=5,n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "# performing predictions on the test dataset\n",
    "y_pred = rfc.predict(X_test)\n",
    "# using metrics module for accuracy calculation\n",
    "accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"RandomForest{}\".format(accuracy))\n",
    "\n",
    "# Modeling\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"DecisionTre{}\".format(accuracy))\n",
    "# print('Accuracy: {:.2f} %'.format(accuracy*100))\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
