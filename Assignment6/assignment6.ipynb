{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master DSLS / Programming 3 / Assignment 6\n",
    "# Final Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "https://bioinf.nl/~martijn/master/programming3/assignment6.html\n",
    "\n",
    "This is the final for programming 3. In this assignment, I will develop scikit-learn machine learning models to predict the function of the proteins in the specific dataset. This model will use small InterPro_annotations_accession to predict large InterPro_annotations_accession.\n",
    "The definition of small InterPro_annotations_accession and large InterPro_annotations_accession is defined as below:\n",
    "\n",
    "If InterPro_annotations_accession's feature length(Stop_location-Start_location) / Sequence_length > 0.9, it is large InterPro_annotations_accession.\n",
    "\n",
    "Otherwise, it is a small InterPro_annotations_accession.\n",
    "\n",
    "We can briefly rewrite as:\n",
    "\n",
    "            |(Stop - Start)|/Sequence >  0.9 --> Large\n",
    "\n",
    "            |(Stop - Start)|/Sequence <= 0.9 --> small\n",
    "\n",
    "I will also check the \"bias\" and \"noise\" that does not make sense from the dataset.\n",
    "\n",
    "ie. lines(-) from the TSV file which don't contain InterPRO numbers\n",
    "\n",
    "ie. proteins which don't have a large feature (according to the criteria above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Goal\n",
    "\n",
    "The goal of this assignment is to predict large InterPro_annotations_accession by small InterPro_annotations_accession.\n",
    "\n",
    "I will use the dataset from /data/dataprocessing/interproscan/all_bacilli.tsv file on assemblix2012 and assemblix2019. However, this file contains ~4,200,000 protein annotations, so I will put a subset of all_bacilli.tsv on GitHub and on local for code testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark web UI:http://localhost:4040/jobs/\n",
    "# Output format : https://interproscan-docs.readthedocs.io/en/latest/OutputFormats.html\n",
    "\n",
    "# pyspark\n",
    "import pyspark\n",
    "import logging\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark_dist_explore import hist\n",
    "\n",
    "# pyspark ML\n",
    "from pyspark.ml.feature import StringIndexer,VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder \n",
    "\n",
    "# ETL & visualization\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "\n",
    "def create_dataframe(path, num_rows=None):\n",
    "    \"\"\"\n",
    "    Create a Spark DataFrame from a file with the specified schema.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path.\n",
    "        num_rows (int): The number of rows to select from the DataFrame (default is None, which processes the entire file).\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: The Spark DataFrame.\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"Protein_accession\", StringType(), True),\n",
    "        StructField(\"Sequence_MD5_digest\", StringType(), True),\n",
    "        StructField(\"Sequence_length\", IntegerType(), True),\n",
    "        StructField(\"Analysis\", StringType(), True),\n",
    "        StructField(\"Signature_accession\", StringType(), True),\n",
    "        StructField(\"Signature_description\", StringType(), True),\n",
    "        StructField(\"Start_location\", IntegerType(), True),\n",
    "        StructField(\"Stop_location\", IntegerType(), True),\n",
    "        StructField(\"Score\", FloatType(), True),\n",
    "        StructField(\"Status\", StringType(), True),\n",
    "        StructField(\"Date\", StringType(), True),\n",
    "        StructField(\"InterPro_annotations_accession\", StringType(), True),\n",
    "        StructField(\"InterPro_annotations_description\", StringType(), True),\n",
    "        StructField(\"GO_annotations\", StringType(), True),\n",
    "        StructField(\"Pathways_annotations\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # Configure logging to store logs in a file\n",
    "    logging.basicConfig(filename='/students/2021-2022/master/DaanSteur_DSLS/spark_logs.txt', level=logging.INFO)\n",
    "\n",
    "    spark = SparkSession.builder.master(\"local[16]\") \\\n",
    "        .config('spark.driver.memory', '128g') \\\n",
    "        .config('spark.executor.memory', '128g') \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "        .appName(\"InterPro\").getOrCreate()\n",
    "\n",
    "    # # Set the Spark log level\n",
    "    # spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    df = spark.read.option(\"sep\", \"\\t\").option(\"header\", \"False\").csv(path, schema=schema)\n",
    "\n",
    "    # If num_rows is None, process the entire file\n",
    "    if num_rows is not None:\n",
    "        df = df.limit(num_rows)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# path = \"/data/dataprocessing/interproscan/all_bacilli.tsv\"\n",
    "# data = create_dataframe(path, num_rows=None)\n",
    "# # get the number of rows\n",
    "# print(\"Number of rows: \", data.count())\n",
    "# data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "from pyspark.sql.functions import col, abs, when, max\n",
    "\n",
    "\n",
    "def data_preprocessing(df):\n",
    "    \"\"\"\n",
    "    It will help you to finish preprocessing data.\n",
    "    df: spark df\n",
    "    return small_df,large_df\n",
    "    \"\"\"\n",
    "    # remove InterPro_annotations_accession == \"-\"\n",
    "    # get the length of protein\n",
    "    # get the ratio to distinguish them to large and small InterPro_annotations_accession\n",
    "    # 1 for large, 0 for small InterPro_annotations_accession\n",
    "    df = df.filter(df.InterPro_annotations_accession != \"-\")\\\n",
    "        .withColumn(\"Ratio\", (abs(df[\"Stop_location\"] - df[\"Start_location\"])/df[\"Sequence_length\"]))\\\n",
    "        .withColumn(\"Size\", when((abs(df[\"Stop_location\"] - df[\"Start_location\"])/df[\"Sequence_length\"])>0.9,1).otherwise(0))\n",
    "\n",
    "    # get the intersection to make sure there is a match of large and small InterPro_annotations_accession(at least one large and one small InterPro_annotations_accession)\n",
    "    intersection = df.filter(df.Size == 0).select(\"Protein_accession\").intersect(df.filter(df.Size == 1).select(\"Protein_accession\"))\n",
    "    intersection_df = intersection.join(df,[\"Protein_accession\"])\n",
    "\n",
    "    # get the number of small InterPro_annotations_accession in each Protein_accession\n",
    "    small_df = intersection_df.filter(df.Size == 0).groupBy([\"Protein_accession\"]).pivot(\"InterPro_annotations_accession\").count()\n",
    "\n",
    "    # There are several InterPro_annotations_accession with the same Protein_accession. I only choose the largest one.\n",
    "    large_df = intersection_df.filter(df.Size == 1).groupby([\"Protein_accession\"]).agg(max(\"Ratio\").alias(\"Ratio\"))\n",
    "    large_df = large_df.join(intersection_df,[\"Protein_accession\",\"Ratio\"],\"inner\").dropDuplicates([\"Protein_accession\"])\n",
    "\n",
    "    # Drop the useless columns\n",
    "    columns = (\"Sequence_MD5_digest\",\"Analysis\",\"Signature_accession\",\"Signature_description\",\n",
    "        \"Score\",\"Status\",\"Date\",\"InterPro_annotations_description\",\"GO_annotations\",\n",
    "        \"Pathways_annotations\",\"Ratio\",\"Size\",\"Stop_location\",\"Start_location\",\"Sequence_length\")\n",
    "    large_df = large_df.drop(*columns)\n",
    "    \n",
    "    print(\"data preprocessing finished\")\n",
    "    return small_df, large_df\n",
    "\n",
    "# Example usage:\n",
    "# small_df, large_df = data_preprocessing(data)\n",
    "\n",
    "# # Show the first few rows of small_df and large_df\n",
    "# print(\"Number of rows in small_df: \", small_df.count())\n",
    "# small_df.show(5)\n",
    "# print(\"Number of rows in small_df: \", large_df.count())\n",
    "# large_df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml df created\n",
    "def ML_df_create(small_df,large_df):\n",
    "    \"\"\"\n",
    "    It will help you to create a correct ML dataframe.\n",
    "    small_df: spark df, preprocessing to fit the criteria ratio<=0.9\n",
    "    large_df: spark df, preprocessing to fit the criteria ratio>0.9\n",
    "    return ML_df\n",
    "    \"\"\"\n",
    "    # Create the df for ML, we do not need Protein_accession anymore.\n",
    "    ML_df = large_df.join(small_df,[\"Protein_accession\"],\"outer\").fillna(0).drop(\"Protein_accession\")\n",
    "\n",
    "    # catalogize y variable\n",
    "    Label = StringIndexer(inputCol=\"InterPro_annotations_accession\", outputCol=\"InterPro_index\")\n",
    "\n",
    "    # catalogize X variable\n",
    "    input_columns = ML_df.columns[1:]\n",
    "    assembler = VectorAssembler(inputCols=input_columns,outputCol=\"InterPro_features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[Label,assembler])\n",
    "    ML_final = pipeline.fit(ML_df).transform(ML_df)\n",
    "    \n",
    "    print(\"ML dataframe created\")\n",
    "    return ML_final\n",
    "\n",
    "# ml_final = ML_df_create(small_df, large_df)\n",
    "# print(\"Number of rows in ml_final: \", ml_final.count())\n",
    "# ml_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "Testing machine learning models to predict large InterPro_annotations_accession by small InterPro_annotations_accession.\n",
    "From earlier papers and communication was found that naivebayes was a strong contenteder for this problem. and therefore i will continue using this model with further testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting machine learningdata into training and test sets\n",
    "def split_data(ML_final, percentage=0.7, seed=42):\n",
    "    \"\"\"\n",
    "    Split the data into training and test sets.\n",
    "\n",
    "    Args:\n",
    "        ML_final (DataFrame): The DataFrame to be split.\n",
    "        percentage (float): The percentage of data to be allocated for training (default is 0.7).\n",
    "        seed (int): The random seed for reproducibility (default is 42).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame, DataFrame: The training and test DataFrames.\n",
    "    \"\"\"\n",
    "    trainData, testData = ML_final.randomSplit([percentage, 1 - percentage], seed=seed)\n",
    "    \n",
    "    print(f\"Number of rows in trainData: {trainData.count()}, Number of rows in testData: {testData.count()}\")\n",
    "    \n",
    "    print(\"Data split into training and test sets\")\n",
    "    return trainData, testData\n",
    "\n",
    "\n",
    "# train_data, test_data = split_data(ml_final,percentage=0.7)\n",
    "# train_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# from pyspark.ml.classification import NaiveBayes\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "def train_and_evaluate_naive_bayes_with_cv(model_type, train_data, test_data, output_file):\n",
    "    # Create the NaiveBayes model with the specified model type\n",
    "    nb_model = NaiveBayes(modelType=model_type, labelCol=\"InterPro_index\",\n",
    "                          featuresCol=\"InterPro_features\", predictionCol=\"prediction\")\n",
    "\n",
    "    # Define hyperparameter grid for smoothing\n",
    "    param_grid = (ParamGridBuilder()\n",
    "                  .addGrid(nb_model.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0])\n",
    "                  .build())\n",
    "\n",
    "    # Create a MulticlassClassificationEvaluator\n",
    "    nb_evaluator = MulticlassClassificationEvaluator(labelCol='InterPro_index',\n",
    "                                                    predictionCol='prediction',\n",
    "                                                    metricName='accuracy')\n",
    "\n",
    "    # Create a CrossValidator\n",
    "    cv = CrossValidator(estimator=nb_model,\n",
    "                        evaluator=nb_evaluator,\n",
    "                        estimatorParamMaps=param_grid,\n",
    "                        numFolds=5,\n",
    "                        parallelism=10,\n",
    "                        seed=42)\n",
    "\n",
    "    # Train the model and measure the time taken\n",
    "    start_time = time.time()\n",
    "    cv_model = cv.fit(train_data)\n",
    "    end_time = time.time()\n",
    "    training_time_hours = (end_time - start_time) / 3600\n",
    "\n",
    "    # Make predictions on the test data using the best model from cross-validation\n",
    "    nb_cv_predictions = cv_model.transform(test_data)\n",
    "\n",
    "    # Evaluate the model's accuracy\n",
    "    nb_cv_accuracy = nb_evaluator.evaluate(nb_cv_predictions)\n",
    "\n",
    "    # Train a separate model on the training data (without cross-validation) to get the initial accuracy\n",
    "    nb_initial_model = nb_model.fit(train_data)\n",
    "    nb_initial_predictions = nb_initial_model.transform(test_data)\n",
    "    nb_initial_accuracy = nb_evaluator.evaluate(nb_initial_predictions)\n",
    "\n",
    "    # Write accuracies and time taken to a text file\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\"Model Type: {model_type}\\n\")\n",
    "        f.write(f\"Initial Accuracy: {nb_initial_accuracy}\\n\")\n",
    "        f.write(f\"Cross-Validated Accuracy: {nb_cv_accuracy}\\n\")\n",
    "        f.write(f\"Time Taken (hours): {training_time_hours}\\n\")\n",
    "\n",
    "    print(\"Model Type:\", model_type)\n",
    "    print(\"Initial Accuracy:\", nb_initial_accuracy)\n",
    "    print(\"Cross-Validated Accuracy:\", nb_cv_accuracy)\n",
    "    print(\"Time Taken (hours):\", training_time_hours)\n",
    "    \n",
    "    return cv_model, nb_model\n",
    "\n",
    "# # Example usage:\n",
    "# cv_model, nb_model = train_and_evaluate_naive_bayes_with_cv(\"multinomial\", train_data, test_data, \"nb_multinomial_cv_results.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "def save_spark_model(model, file_path):\n",
    "    \"\"\"\n",
    "    Save a Spark MLlib model to a file.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Spark MLlib model.\n",
    "        file_path (str): File path where the model will be saved.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.save(file_path)\n",
    "    print(f\"Model saved to {file_path}\")\n",
    "        \n",
    "# save_spark_model(cv_model, \"cv_multinomial_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_dataframe_as_csv(dataframe, file_path):\n",
    "    \"\"\"\n",
    "    Save a Spark DataFrame as a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: Spark DataFrame to be saved.\n",
    "        file_path (str): File path where the CSV file will be saved.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Convert Spark DataFrame to Pandas DataFrame\n",
    "    pandas_df = dataframe.toPandas()\n",
    "    \n",
    "    # Save Pandas DataFrame as CSV\n",
    "    pandas_df.to_csv(file_path, index=False)\n",
    "    print(f\"DataFrame saved to {file_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a Spark DataFrame 'train_data' and want to save it as 'train_data.csv'\n",
    "# Replace 'train_data' and 'train_data.csv' with your DataFrame and desired file path.\n",
    "# save_dataframe_as_csv(train_data, 'train_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/17 13:09:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/17 13:09:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocessing finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML dataframe created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/17 13:10:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in trainData: 301, Number of rows in testData: 101\n",
      "Data split into training and test sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/17 13:13:48 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:13:49 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:13:49 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:13:49 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:13:49 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:13:49 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:13:49 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:13:49 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:13:51 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:13:51 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:13:51 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:13:51 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:13:51 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:13:51 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:13:51 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:13:51 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:13:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:13:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:13:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:13:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:13:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:13:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:13:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:13:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:17:28 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:17:28 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:17:28 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:17:28 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:17:28 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:17:28 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:17:28 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:17:28 WARN DAGScheduler: Broadcasting large task binary with size 1117.1 KiB\n",
      "23/09/17 13:17:29 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:17:29 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:17:29 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:17:29 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:17:29 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:17:29 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:17:29 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:17:30 WARN DAGScheduler: Broadcasting large task binary with size 1115.6 KiB\n",
      "23/09/17 13:17:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:17:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:17:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:17:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:17:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:17:32 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:17:33 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/09/17 13:17:33 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    path = \"/data/dataprocessing/interproscan/all_bacilli.tsv\"\n",
    "    data = create_dataframe(path, num_rows=10000)\n",
    "    small_df, large_df = data_preprocessing(data)\n",
    "    ml_final = ML_df_create(small_df, large_df)\n",
    "    train_data, test_data = split_data(ml_final,percentage=0.7)\n",
    "    cv_model, nb_model = train_and_evaluate_naive_bayes_with_cv(\"multinomial\", train_data, test_data, \"/students/2021-2022/master/DaanSteur_DSLS/nb_multinomial_cv_results.txt\")\n",
    "    save_spark_model(nb_model, '/students/2021-2022/master/DaanSteur_DSLS/nb_model_multinomial.pkl')\n",
    "    save_spark_model(cv_model, '/students/2021-2022/master/DaanSteur_DSLS/cv_model_multinomial.pkl')\n",
    "    \n",
    "    cv_model, nb_model = train_and_evaluate_naive_bayes_with_cv(\"gaussian\", train_data, test_data, \"/students/2021-2022/master/DaanSteur_DSLS/nb_gaussian_cv_results.txt\")\n",
    "    save_spark_model(nb_model, '/students/2021-2022/master/DaanSteur_DSLS/nb_model_gaussian.pkl')\n",
    "    save_spark_model(cv_model, '/students/2021-2022/master/DaanSteur_DSLS/cv_model_gaussian.pkl')\n",
    "    \n",
    "    save_dataframe_as_csv(train_data, '/students/2021-2022/master/DaanSteur_DSLS/train_data.csv')\n",
    "    save_dataframe_as_csv(test_data, '/students/2021-2022/master/DaanSteur_DSLS/test_data.csv')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "\n",
    "# # Assuming you have a PySpark DataFrame named large_proteins and small_proteins\n",
    "# # Select the feature_Length column and convert it to a NumPy array\n",
    "# large_feature_lengths = large_proteins.select('feature_Length').rdd.flatMap(lambda x: x).collect()\n",
    "# small_feature_lengths = small_proteins.select('feature_Length').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# # Create subplots with a shared y-axis in a 2x2 grid\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(12, 10), sharey='row')\n",
    "\n",
    "# # Customize the style of the histograms\n",
    "# color_large = 'lightgreen'\n",
    "# color_small = 'orange'\n",
    "# bins = 20\n",
    "\n",
    "# # Plot for small_proteins\n",
    "# axs[0, 0].set_title('Histogram showing the feature length of small Interpro accession')\n",
    "# axs[0, 0].set_xlabel('Feature length (Start-Stop)')\n",
    "# axs[0, 0].set_ylabel('Frequency')\n",
    "# axs[0, 0].hist(small_feature_lengths, bins=bins, color=color_small, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# # Plot for large_proteins\n",
    "# axs[0, 1].set_title('Histogram showing the feature length of large Interpro accession')\n",
    "# axs[0, 1].set_xlabel('Feature length (Start-Stop)')\n",
    "# axs[0, 1].set_ylabel('Frequency')\n",
    "# axs[0, 1].hist(large_feature_lengths, bins=bins, color=color_large, edgecolor='black', alpha=0.7)\n",
    "\n",
    "\n",
    "\n",
    "# # Add gridlines\n",
    "# for ax in axs[:, 0]:\n",
    "#     ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# # Convert PySpark DataFrames to Pandas DataFrames\n",
    "# large_df = large_proteins.toPandas()\n",
    "# small_df = small_proteins.toPandas()\n",
    "\n",
    "# # Set Seaborn theme and style\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# # Plot for small_proteins with light green color\n",
    "# sns.boxplot(x=small_df[\"feature_Length\"], ax=axs[1, 1], color='lightgreen')\n",
    "# axs[1, 0].set_title('Boxplot of small Interpro accessions')\n",
    "# axs[1, 0].set_ylabel('Feature Length')\n",
    "\n",
    "# # Plot for large_proteins with orange color\n",
    "# sns.boxplot(x=large_df[\"feature_Length\"], ax=axs[1, 0], color='orange')\n",
    "# axs[1, 1].set_title('Boxplot of large Interpro accessions')\n",
    "# axs[1, 1].set_ylabel('Feature Length')\n",
    "\n",
    "# # Adjust spacing between subplots\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # plt.savefig('protein_analys_plots.png')\n",
    "\n",
    "# # Show the combined plot\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
