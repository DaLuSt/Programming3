{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master DSLS / Programming 3 / Assignment 6\n",
    "# Final Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "https://bioinf.nl/~martijn/master/programming3/assignment6.html\n",
    "\n",
    "This is the final for programming 3. In this assignment, I will develop scikit-learn machine learning models to predict the function of the proteins in the specific dataset. This model will use small InterPro_annotations_accession to predict large InterPro_annotations_accession.\n",
    "The definition of small InterPro_annotations_accession and large InterPro_annotations_accession is defined as below:\n",
    "\n",
    "If InterPro_annotations_accession's feature length(Stop_location-Start_location) / Sequence_length > 0.9, it is large InterPro_annotations_accession.\n",
    "\n",
    "Otherwise, it is a small InterPro_annotations_accession.\n",
    "\n",
    "We can briefly rewrite as:\n",
    "\n",
    "            |(Stop - Start)|/Sequence >  0.9 --> Large\n",
    "\n",
    "            |(Stop - Start)|/Sequence <= 0.9 --> small\n",
    "\n",
    "I will also check the \"bias\" and \"noise\" that does not make sense from the dataset.\n",
    "\n",
    "ie. lines(-) from the TSV file which don't contain InterPRO numbers\n",
    "\n",
    "ie. proteins which don't have a large feature (according to the criteria above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Goal\n",
    "\n",
    "The goal of this assignment is to predict large InterPro_annotations_accession by small InterPro_annotations_accession.\n",
    "\n",
    "I will use the dataset from /data/dataprocessing/interproscan/all_bacilli.tsv file on assemblix2012 and assemblix2019. However, this file contains ~4,200,000 protein annotations, so I will put a subset of all_bacilli.tsv on GitHub and on local for code testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/14 15:45:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Spark web UI:http://localhost:4040/jobs/\n",
    "# Output format : https://interproscan-docs.readthedocs.io/en/latest/OutputFormats.html\n",
    "\n",
    "# pyspark\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark_dist_explore import hist\n",
    "\n",
    "# pyspark ML\n",
    "from pyspark.ml.feature import StringIndexer,VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes, GBTClassifier, MultilayerPerceptronClassifier, LinearSVC\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder \n",
    "\n",
    "# ETL & visualization\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# call spark session\n",
    "spsession = SparkContext('local[16]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-----+----+------------------+--------------+------------------+----+\n",
      "|             Protein|Sequence_length|Start|Stop|Interpro_accession|feature_Length|             ratio|Size|\n",
      "+--------------------+---------------+-----+----+------------------+--------------+------------------+----+\n",
      "|gi|29896607|gb|AA...|            429|    1| 429|         IPR006264|         428.0|0.9976689976689976|   1|\n",
      "|gi|29895450|gb|AA...|            419|    1| 419|         IPR016496|         418.0|0.9976133651551312|   1|\n",
      "|gi|29894872|gb|AA...|            412|    1| 412|         IPR017568|         411.0|0.9975728155339806|   1|\n",
      "|gi|29894872|gb|AA...|            412|    1| 412|         IPR016039|         411.0|0.9975728155339806|   1|\n",
      "|gi|29897784|gb|AA...|            372|    1| 372|         IPR010162|         371.0|0.9973118279569892|   1|\n",
      "|gi|29896081|gb|AA...|            350|    1| 350|         IPR024169|         349.0|0.9971428571428571|   1|\n",
      "|gi|29895876|gb|AA...|            318|    1| 318|         IPR045097|         317.0|0.9968553459119497|   1|\n",
      "|gi|29894215|gb|AA...|            506|    1| 505|         IPR011167|         504.0|0.9960474308300395|   1|\n",
      "|gi|29897469|gb|AA...|            240|    1| 240|         IPR036393|         239.0|0.9958333333333333|   1|\n",
      "|gi|29897469|gb|AA...|            240|    1| 240|         IPR011817|         239.0|0.9958333333333333|   1|\n",
      "|gi|29898007|gb|AA...|            231|    1| 231|         IPR035994|         230.0|0.9956709956709957|   1|\n",
      "|gi|29895450|gb|AA...|            419|    1| 418|         IPR016496|         417.0|0.9952267303102625|   1|\n",
      "|gi|29897323|gb|AA...|            179|    1| 179|         IPR007300|         178.0| 0.994413407821229|   1|\n",
      "|gi|29897105|gb|AA...|            347|    1| 346|         IPR036188|         345.0|0.9942363112391931|   1|\n",
      "|gi|29893889|gb|AA...|            153|    1| 153|         IPR008463|         152.0|0.9934640522875817|   1|\n",
      "|gi|29894480|gb|AA...|            139|    1| 139|         IPR012187|         138.0|0.9928057553956835|   1|\n",
      "|gi|29894872|gb|AA...|            412|    2| 411|         IPR000794|         409.0|0.9927184466019418|   1|\n",
      "|gi|29898905|gb|AA...|            131|    1| 131|         IPR039072|         130.0|0.9923664122137404|   1|\n",
      "|gi|29893947|gb|AA...|            120|    1| 120|         IPR036373|         119.0|0.9916666666666667|   1|\n",
      "|gi|29893947|gb|AA...|            120|    1| 120|         IPR000456|         119.0|0.9916666666666667|   1|\n",
      "+--------------------+---------------+-----+----+------------------+--------------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o122.fit.\n: org.apache.spark.SparkException: Input column InterPro_annotations_accession does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbioinf.nl/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trainData, testData\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbioinf.nl/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m small_df, large_df \u001b[39m=\u001b[39m data_preprocessing(processed_df)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bbioinf.nl/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m ML_final \u001b[39m=\u001b[39m ML_df_create(small_df,large_df)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbioinf.nl/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m train_data, test_Data \u001b[39m=\u001b[39m split_data(ML_final,percentage\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbioinf.nl/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m train_data\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;32m/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbioinf.nl/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m assembler \u001b[39m=\u001b[39m VectorAssembler(inputCols\u001b[39m=\u001b[39minput_columns,outputCol\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInterPro_features\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbioinf.nl/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39m[Label,assembler])\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bbioinf.nl/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m ML_final \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit(ML_df)\u001b[39m.\u001b[39mtransform(ML_df)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bbioinf.nl/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ML_final\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    135\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o122.fit.\n: org.apache.spark.SparkException: Input column InterPro_annotations_accession does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "def process_protein_data(path):\n",
    "    # Initialize SparkSession\n",
    "    spark = SparkSession.builder.appName(\"ProteinDataProcessing\").getOrCreate()\n",
    "\n",
    "    # Read the data from the given path\n",
    "    df = spark.read.csv(path, sep='\\t', header=None)\n",
    "    \n",
    "    # Drop the first column \"_c0\"\n",
    "    df = df.drop(\"_c0\")\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.toDF(\n",
    "        \"Protein\", \"Sequence_MD5_digest\", \"Sequence_length\", \"Analysis\",\n",
    "        \"Signature_accession\", \"Signature_description\", \"Start\", \"Stop\",\n",
    "        \"Score\", \"Status\", \"Date\", \"Interpro_accession\",\n",
    "        \"Interpro_description\", \"GO\", \"Pathway\"\n",
    "    )\n",
    "\n",
    "    # Remove rows where Interpro_accession == \"-\"\n",
    "    df = df.filter(df[\"Interpro_accession\"] != \"-\")\n",
    "\n",
    "    # Calculate feature_Length\n",
    "    df = df.withColumn(\"feature_Length\", (df[\"Stop\"] - df[\"Start\"]))\n",
    "\n",
    "    # Calculate the ratio\n",
    "    df = df.withColumn(\"ratio\", (df[\"feature_Length\"] / df[\"Sequence_length\"]))\n",
    "\n",
    "    # Assign Size (1 for large, 0 for small)\n",
    "    df = df.withColumn(\"Size\", when(df[\"ratio\"] > 0.9, 1).otherwise(0))\n",
    "\n",
    "    # Select columns to drop\n",
    "    to_drop = [\"Sequence_MD5_digest\", \"Analysis\", \"Signature_accession\", \"Signature_description\",\n",
    "               \"Score\", \"Status\", \"Date\", \"Interpro_description\", \"GO\", \"Pathway\"]\n",
    "\n",
    "    # Drop selected columns\n",
    "    df = df.drop(*to_drop)\n",
    "    \n",
    "    # drop duplicates\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # Sort by ratio from big to small\n",
    "    df = df.orderBy(df.ratio.desc())\n",
    "\n",
    "    return df\n",
    "\n",
    "path = \"/homes/dlsteur/Git_repos/Programming3/Assignment6/all_bacilli.tsv\"\n",
    "processed_df = process_protein_data(path)\n",
    "\n",
    "# Show the processed DataFrame\n",
    "processed_df.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_preprocessing(df):\n",
    "    \"\"\"\n",
    "    It will help you to finish preprocessing data.\n",
    "    df: spark df\n",
    "    return small_df,large_df\n",
    "    \"\"\"\n",
    "    # remove InterPro_annotations_accession == \"-\"\n",
    "    # get the length of protein\n",
    "    # get the ratio to distinguish them to large and small InterPro_annotations_accession\n",
    "    # 1 for large, 0 for small InterPro_annotations_accession\n",
    "\n",
    "    # get the intersection to make sure there is a match of large and small InterPro_annotations_accession(at least one large and one small InterPro_annotations_accession)\n",
    "    intersection = df.filter(df.Size == 0).select(\"protein\").intersect(df.filter(df.Size == 1).select(\"protein\"))\n",
    "    intersection_df = intersection.join(df,[\"protein\"])\n",
    "\n",
    "    # get the number of small InterPro_annotations_accession in each protein\n",
    "    small_df = intersection_df.filter(df.Size == 0).groupBy([\"protein\"]).pivot(\"InterPro_accession\").count()\n",
    "\n",
    "    # There are several InterPro_annotations_accession with the same protein. I only choose the largest one.\n",
    "    large_df = intersection_df.filter(df.Size == 1).groupby([\"protein\"]).agg(max(\"Ratio\").alias(\"Ratio\"))\n",
    "    large_df = large_df.join(intersection_df,[\"protein\",\"Ratio\"],\"inner\").dropDuplicates([\"protein\"])\n",
    "\n",
    "    # Drop the useless columns\n",
    "    columns = (\"Sequence_MD5_digest\",\"Analysis\",\"Signature_accession\",\"Signature_description\",\n",
    "        \"Score\",\"Status\",\"Date\",\"InterPro_annotations_description\",\"GO_annotations\",\n",
    "        \"Pathways_annotations\",\"Ratio\",\"Size\",\"Stop_location\",\"Start_location\",\"Sequence_length\")\n",
    "    large_df = large_df.drop(*columns)\n",
    "    return small_df, large_df\n",
    "\n",
    "def ML_df_create(small_df,large_df):\n",
    "    \"\"\"\n",
    "    It will help you to create a correct ML dataframe.\n",
    "    small_df: spark df, preprocessing to fit the criteria ratio<=0.9\n",
    "    large_df: spark df, preprocessing to fit the criteria ratio>0.9\n",
    "    return ML_df\n",
    "    \"\"\"\n",
    "    # Create the df for ML, we do not need protein anymore.\n",
    "    ML_df = large_df.join(small_df,[\"protein\"],\"outer\").fillna(0).drop(\"protein\")\n",
    "\n",
    "    # catalogize y variable\n",
    "    Label = StringIndexer(inputCol=\"InterPro_annotations_accession\", outputCol=\"InterPro_index\")\n",
    "\n",
    "    # catalogize X variable\n",
    "    input_columns = ML_df.columns[1:]\n",
    "    assembler = VectorAssembler(inputCols=input_columns,outputCol=\"InterPro_features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[Label,assembler])\n",
    "    ML_final = pipeline.fit(ML_df).transform(ML_df)\n",
    "    return ML_final\n",
    "\n",
    "def split_data(ML_final,percentage=0.7):\n",
    "    \"\"\"\n",
    "    it can help you split the data to trainning data and test data.\n",
    "    ML_final: df\n",
    "    percentage:int, you can set another value.\n",
    "    return: trainData, df; testData,df\n",
    "    \"\"\"\n",
    "    (trainData, testData) = ML_final.randomSplit([percentage, 1-percentage],seed=42)\n",
    "    return trainData, testData\n",
    "\n",
    "\n",
    "small_df, large_df = data_preprocessing(processed_df)\n",
    "ML_final = ML_df_create(small_df,large_df)\n",
    "train_data, test_Data = split_data(ML_final,percentage=0.7)\n",
    "train_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intersection to make sure there is a match of large and small InterPro_annotations_accession(at least one large and one small InterPro_annotations_accession)\n",
    "intersection = processed_df.filter(processed_df.Size == 0).select(\"Protein\").intersect(processed_df.filter(processed_df.Size == 1).select(\"Protein\"))\n",
    "intersection_df = intersection.join(processed_df,[\"Protein\"])\n",
    "# intersection_df.toPandas()\n",
    "\n",
    "# select small proteins in df\n",
    "small_proteins = intersection_df.filter(intersection_df.ratio < 0.9)\n",
    "\n",
    "# select latrge proteins in df\n",
    "large_proteins = intersection_df.filter(intersection_df.ratio >= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a PySpark DataFrame named large_proteins and small_proteins\n",
    "# Select the feature_Length column and convert it to a NumPy array\n",
    "large_feature_lengths = large_proteins.select('feature_Length').rdd.flatMap(lambda x: x).collect()\n",
    "small_feature_lengths = small_proteins.select('feature_Length').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create subplots with a shared y-axis in a 2x2 grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10), sharey='row')\n",
    "\n",
    "# Customize the style of the histograms\n",
    "color_large = 'lightgreen'\n",
    "color_small = 'orange'\n",
    "bins = 20\n",
    "\n",
    "# Plot for small_proteins\n",
    "axs[0, 0].set_title('Histogram showing the feature length of small Interpro accession')\n",
    "axs[0, 0].set_xlabel('Feature length (Start-Stop)')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "axs[0, 0].hist(small_feature_lengths, bins=bins, color=color_small, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Plot for large_proteins\n",
    "axs[0, 1].set_title('Histogram showing the feature length of large Interpro accession')\n",
    "axs[0, 1].set_xlabel('Feature length (Start-Stop)')\n",
    "axs[0, 1].set_ylabel('Frequency')\n",
    "axs[0, 1].hist(large_feature_lengths, bins=bins, color=color_large, edgecolor='black', alpha=0.7)\n",
    "\n",
    "\n",
    "\n",
    "# Add gridlines\n",
    "for ax in axs[:, 0]:\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Convert PySpark DataFrames to Pandas DataFrames\n",
    "large_df = large_proteins.toPandas()\n",
    "small_df = small_proteins.toPandas()\n",
    "\n",
    "# Set Seaborn theme and style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Plot for small_proteins with light green color\n",
    "sns.boxplot(x=small_df[\"feature_Length\"], ax=axs[1, 1], color='lightgreen')\n",
    "axs[1, 0].set_title('Boxplot of small Interpro accessions')\n",
    "axs[1, 0].set_ylabel('Feature Length')\n",
    "\n",
    "# Plot for large_proteins with orange color\n",
    "sns.boxplot(x=large_df[\"feature_Length\"], ax=axs[1, 0], color='orange')\n",
    "axs[1, 1].set_title('Boxplot of large Interpro accessions')\n",
    "axs[1, 1].set_ylabel('Feature Length')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('protein_analys_plots.png')\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "### testing four different machine learning models to predict large InterPro_annotations_accession by small InterPro_annotations_accession : NaiveBayes, randomforrst, MultilayerPerceptronClassifier, LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe for ml\n",
    "ml_df = processed_df.select(\"Protein\",\"Interpro_accession\",\"start\",\"stop\",\"feature_Length\",\"ratio\",\"Size\")\n",
    "# ml_df.show()\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Assuming you have a DataFrame named ml_df with columns 'Interpro_accession', 'start', 'stop', 'feature_Length', 'ratio', and 'Size'\n",
    "\n",
    "# Define the target column and feature columns\n",
    "target = \"Interpro_accession\"\n",
    "feature_columns = [\"start\", \"stop\", \"feature_Length\", \"ratio\", \"Size\"]\n",
    "\n",
    "# Cast the relevant columns to DoubleType\n",
    "for col_name in feature_columns:\n",
    "    ml_df = ml_df.withColumn(col_name, ml_df[col_name].cast(DoubleType()))\n",
    "\n",
    "# Assemble feature vectors using VectorAssembler\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "ml_df = vector_assembler.transform(ml_df)\n",
    "\n",
    "# Select the assembled features and the target column\n",
    "data = ml_df.select(\"features\", target)\n",
    "\n",
    "# Split the data into training and test sets (70% for training, 30% for testing)\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Show the first few rows of the training data and test data\n",
    "train_data.show()\n",
    "test_data.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def data_preprocessing(df):\n",
    "    \"\"\"\n",
    "    It will help you to finish preprocessing data.\n",
    "    df: spark df\n",
    "    return small_df,large_df\n",
    "    \"\"\"\n",
    "    # remove InterPro_annotations_accession == \"-\"\n",
    "    # get the length of protein\n",
    "    # get the ratio to distinguish them to large and small InterPro_annotations_accession\n",
    "    # 1 for large, 0 for small InterPro_annotations_accession\n",
    "\n",
    "    # get the intersection to make sure there is a match of large and small InterPro_annotations_accession(at least one large and one small InterPro_annotations_accession)\n",
    "    intersection = df.filter(df.Size == 0).select(\"protein\").intersect(df.filter(df.Size == 1).select(\"protein\"))\n",
    "    intersection_df = intersection.join(df,[\"protein\"])\n",
    "\n",
    "    # get the number of small InterPro_annotations_accession in each protein\n",
    "    small_df = intersection_df.filter(df.Size == 0).groupBy([\"protein\"]).pivot(\"InterPro_accession\").count()\n",
    "\n",
    "    # There are several InterPro_annotations_accession with the same protein. I only choose the largest one.\n",
    "    large_df = intersection_df.filter(df.Size == 1).groupby([\"protein\"]).agg(max(\"Ratio\").alias(\"Ratio\"))\n",
    "    large_df = large_df.join(intersection_df,[\"protein\",\"Ratio\"],\"inner\").dropDuplicates([\"protein\"])\n",
    "\n",
    "    # Drop the useless columns\n",
    "    columns = (\"Sequence_MD5_digest\",\"Analysis\",\"Signature_accession\",\"Signature_description\",\n",
    "        \"Score\",\"Status\",\"Date\",\"InterPro_annotations_description\",\"GO_annotations\",\n",
    "        \"Pathways_annotations\",\"Ratio\",\"Size\",\"Stop_location\",\"Start_location\",\"Sequence_length\")\n",
    "    large_df = large_df.drop(*columns)\n",
    "    return small_df, large_df\n",
    "\n",
    "def ML_df_create(small_df,large_df):\n",
    "    \"\"\"\n",
    "    It will help you to create a correct ML dataframe.\n",
    "    small_df: spark df, preprocessing to fit the criteria ratio<=0.9\n",
    "    large_df: spark df, preprocessing to fit the criteria ratio>0.9\n",
    "    return ML_df\n",
    "    \"\"\"\n",
    "    # Create the df for ML, we do not need protein anymore.\n",
    "    ML_df = large_df.join(small_df,[\"protein\"],\"outer\").fillna(0).drop(\"protein\")\n",
    "\n",
    "    # catalogize y variable\n",
    "    Label = StringIndexer(inputCol=\"InterPro_annotations_accession\", outputCol=\"InterPro_index\")\n",
    "\n",
    "    # catalogize X variable\n",
    "    input_columns = ML_df.columns[1:]\n",
    "    assembler = VectorAssembler(inputCols=input_columns,outputCol=\"InterPro_features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[Label,assembler])\n",
    "    ML_final = pipeline.fit(ML_df).transform(ML_df)\n",
    "    return ML_final\n",
    "\n",
    "def split_data(ML_final,percentage=0.7):\n",
    "    \"\"\"\n",
    "    it can help you split the data to trainning data and test data.\n",
    "    ML_final: df\n",
    "    percentage:int, you can set another value.\n",
    "    return: trainData, df; testData,df\n",
    "    \"\"\"\n",
    "    (trainData, testData) = ML_final.randomSplit([percentage, 1-percentage],seed=42)\n",
    "    return trainData, testData\n",
    "\n",
    "\n",
    "small_df, large_df = data_preprocessing(processed_df)\n",
    "ML_final = ML_df_create(small_df,large_df)\n",
    "train_data, test_Data = split_data(ML_final,percentage=0.7)\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create a NaiveBayes model\n",
    "nb = NaiveBayes(featuresCol=\"features\", smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# Define a parameter grid for grid search\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.1, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define a cross-validator with 5-fold cross-validation\n",
    "crossval = CrossValidator(estimator=nb,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "                          numFolds=5,\n",
    "                          seed=42)\n",
    "\n",
    "# Run cross-validation and choose the best set of parameters\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "best_model = cv_model.bestModel\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Evaluate the accuracy of the best model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save file\n",
    "# trainFile = '/students/2021-2022/master/DaanSteur_DSLS/trainData.pkl'\n",
    "# trainData.toPandas().set_index('InterPro_annotations_accession').to_pickle(trainFile)\n",
    "# testFile = '/students/2021-2022/master/DaanSteur_DSLS/testData.pkl'\n",
    "# testData.toPandas().set_index('InterPro_annotations_accession').to_pickle(testFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model\n",
    "# nb_cvModel.bestModel.write().overwrite().save(\"/students/2021-2022/master/DaanSteur_DSLS/NaiveBayesBestModel\")\n",
    "# dtc_cvModel.bestModel.write().overwrite().save(\"/students/2021-2022/master/DaanSteur_DSLS/DecisionTreeBestModel\")\n",
    "# rf_cvModel.bestModel.write().overwrite().save(\"/students/2021-2022/master/DaanSteur_DSLS/RandomForestBestModel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
