{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/usr/local/lib/python3.9/dist-packages/pandas/core/arrays/masked.py:64: UserWarning: Pandas requires version '1.3.2' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# import libaries\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType,FloatType\n",
    "import pyspark.sql.functions as f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output folder\n",
      "Starting session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/10 22:23:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in file\n",
      "File loaded\n",
      "root\n",
      " |-- Protein_accession: string (nullable = true)\n",
      " |-- Sequence_MD5_digest: string (nullable = true)\n",
      " |-- Sequence_length: integer (nullable = true)\n",
      " |-- Analysis: string (nullable = true)\n",
      " |-- Signature_accession: string (nullable = true)\n",
      " |-- Signature_description: string (nullable = true)\n",
      " |-- Start_location: integer (nullable = true)\n",
      " |-- Stop_location: integer (nullable = true)\n",
      " |-- Score: float (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- InterPro_annotations_accession: string (nullable = true)\n",
      " |-- InterPro_annotations_description: string (nullable = true)\n",
      " |-- GO_annotations: string (nullable = true)\n",
      " |-- Pathways_annotations: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PSTool:\n",
    "    def __init__(self):\n",
    "        print('Creating output folder')\n",
    "        if os.path.exists('output'):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs('output')\n",
    "\n",
    "    def pyspark_session(self, host_location):\n",
    "        \"\"\"\n",
    "        Creates and returns spark session object\n",
    "        \"\"\"\n",
    "        print('Starting session')\n",
    "        sc = SparkContext(host_location)  # Create spark context\n",
    "        spark = SparkSession(sc)  # Create session\n",
    "        return spark\n",
    "\n",
    "    def file_loader(self, path, delim, spark_obj, schema):\n",
    "        print('Loading in file')\n",
    "        data = spark_obj.read.options(delimiter=delim).option(\"header\",\"False\").csv(path, schema=schema)\n",
    "        \n",
    "        print('File loaded')\n",
    "        return data\n",
    "\n",
    "    def get_questions(self, df):\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pstool = PSTool()  # Instanciate object\n",
    "    spk = pstool.pyspark_session('local[16]')  # start session\n",
    "    # load data\n",
    "    path = '/data/dataprocessing/interproscan/all_bacilli.tsv'\n",
    "    # path = 'all_bacilli_subset.tsv'\n",
    "    schema = StructType([\n",
    "        StructField(\"Protein_accession\", StringType(), True),\n",
    "        StructField(\"Sequence_MD5_digest\", StringType(), True),\n",
    "        StructField(\"Sequence_length\", IntegerType(), True),\n",
    "        StructField(\"Analysis\", StringType(), True),\n",
    "        StructField(\"Signature_accession\", StringType(), True),\n",
    "        StructField(\"Signature_description\", StringType(), True),\n",
    "        StructField(\"Start_location\", IntegerType(), True),\n",
    "        StructField(\"Stop_location\", IntegerType(), True),\n",
    "        StructField(\"Score\", FloatType(), True),\n",
    "        StructField(\"Status\", StringType(), True),\n",
    "        StructField(\"Date\", StringType(), True),\n",
    "        StructField(\"InterPro_annotations_accession\", StringType(), True),\n",
    "        StructField(\"InterPro_annotations_description\", StringType(), True),\n",
    "        StructField(\"GO_annotations\", StringType(), True),\n",
    "        StructField(\"Pathways_annotations\", StringType(), True)])\n",
    "    \n",
    "    df = pstool.file_loader(path, '\\t', spk, schema)\n",
    "#     pstool.get_questions(df)\n",
    "#     print('Closing spark session')\n",
    "#     spk.sparkContext.stop()\n",
    "    df.printSchema()  # Shows column names and some info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "The \"function\" of the protein which is the \"class\" your model should predict is defined as the InterPRO number which covers:\n",
    "- \">\" 90% of the protein's sequenc\n",
    "- Covers the largest length of the sequence\n",
    "\n",
    "As we are to train on the interpro number:\n",
    "- Remove any rows without interpro number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9703\n",
      "len: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 4200591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   Protein_accession|\n",
      "+--------------------+\n",
      "|gi|510143242|gb|A...|\n",
      "|gi|510143242|gb|A...|\n",
      "|gi|510143242|gb|A...|\n",
      "|gi|510143242|gb|A...|\n",
      "|gi|510143242|gb|A...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| Sequence_MD5_digest|\n",
      "+--------------------+\n",
      "|d6f8e49a4de47c68d...|\n",
      "|d6f8e49a4de47c68d...|\n",
      "|d6f8e49a4de47c68d...|\n",
      "|d6f8e49a4de47c68d...|\n",
      "|d6f8e49a4de47c68d...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Sequence_length|\n",
      "+---------------+\n",
      "|           6359|\n",
      "|           6359|\n",
      "|           6359|\n",
      "|           6359|\n",
      "|           6359|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|       Analysis|\n",
      "+---------------+\n",
      "|ProSitePatterns|\n",
      "|ProSitePatterns|\n",
      "|ProSitePatterns|\n",
      "|ProSitePatterns|\n",
      "|ProSitePatterns|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Signature_accession|\n",
      "+-------------------+\n",
      "|            PS00455|\n",
      "|            PS00455|\n",
      "|            PS00455|\n",
      "|            PS00455|\n",
      "|            PS00455|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Signature_description|\n",
      "+---------------------+\n",
      "| Putative AMP-bind...|\n",
      "| Putative AMP-bind...|\n",
      "| Putative AMP-bind...|\n",
      "| Putative AMP-bind...|\n",
      "| Putative AMP-bind...|\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Start_location|\n",
      "+--------------+\n",
      "|          1645|\n",
      "|          5690|\n",
      "|          3144|\n",
      "|          4177|\n",
      "|           604|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Stop_location|\n",
      "+-------------+\n",
      "|         1656|\n",
      "|         5701|\n",
      "|         3155|\n",
      "|         4188|\n",
      "|          615|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Score|\n",
      "+-----+\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Status|\n",
      "+------+\n",
      "|     T|\n",
      "|     T|\n",
      "|     T|\n",
      "|     T|\n",
      "|     T|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|25-04-2022|\n",
      "|25-04-2022|\n",
      "|25-04-2022|\n",
      "|25-04-2022|\n",
      "|25-04-2022|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|InterPro_annotations_accession|\n",
      "+------------------------------+\n",
      "|                     IPR020845|\n",
      "|                     IPR020845|\n",
      "|                     IPR020845|\n",
      "|                     IPR020845|\n",
      "|                     IPR020845|\n",
      "+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|InterPro_annotations_description|\n",
      "+--------------------------------+\n",
      "|            AMP-binding, cons...|\n",
      "|            AMP-binding, cons...|\n",
      "|            AMP-binding, cons...|\n",
      "|            AMP-binding, cons...|\n",
      "|            AMP-binding, cons...|\n",
      "+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|GO_annotations|\n",
      "+--------------+\n",
      "|             -|\n",
      "|             -|\n",
      "|             -|\n",
      "|             -|\n",
      "|             -|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Pathways_annotations|\n",
      "+--------------------+\n",
      "|MetaCyc: PWY-1061...|\n",
      "|MetaCyc: PWY-1061...|\n",
      "|MetaCyc: PWY-1061...|\n",
      "|MetaCyc: PWY-1061...|\n",
      "|MetaCyc: PWY-1061...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:======================================================> (81 + 3) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                perc|\n",
      "+--------------------+\n",
      "|0.001729831734549...|\n",
      "|0.001729831734549...|\n",
      "|0.001729831734549...|\n",
      "|0.001729831734549...|\n",
      "|0.001729831734549...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# remove rows that do not have Interpro number\n",
    "print(df.select('InterPro_annotations_accession').distinct().count())\n",
    "IPRO_filt = df.filter(df[\"InterPro_annotations_accession\"] != '-')\n",
    "print(IPRO_filt.select('InterPro_annotations_accession').distinct().count())\n",
    "\n",
    "# check amount of rows left. \n",
    "print('len:', len(df.columns))\n",
    "print('count:' , df.count())\n",
    "# check if the columns are propperly loaded in. \n",
    "df_sizes = IPRO_filt.withColumn('perc', abs(df.Start_location - df.Stop_location) / df.Sequence_length).sort('perc')\n",
    "for i in df_sizes.columns:\n",
    "    print(df_sizes.select(i).show(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InterPro_annotations_accession']\n",
      "DataFrame[InterPro_annotations_accession: string]\n",
      "['Protein_accession', 'Sequence_MD5_digest', 'Sequence_length', 'Analysis', 'Signature_accession', 'Signature_description', 'Start_location', 'Stop_location', 'Score', 'Status', 'Date', 'InterPro_annotations_description', 'GO_annotations', 'Pathways_annotations']\n",
      "DataFrame[Protein_accession: string, Sequence_MD5_digest: string, Sequence_length: int, Analysis: string, Signature_accession: string, Signature_description: string, Start_location: int, Stop_location: int, Score: float, Status: string, Date: string, InterPro_annotations_description: string, GO_annotations: string, Pathways_annotations: string]\n"
     ]
    }
   ],
   "source": [
    "# Organize data into labels for easier use\n",
    "label_names = [\"InterPro_annotations_accession\"]\n",
    "labels = IPRO_filt.select(label_names)\n",
    "feature_names = [i for i in IPRO_filt.columns if i not in label_names]\n",
    "features = IPRO_filt.select(feature_names)\n",
    "\n",
    "print(label_names)\n",
    "print(labels)\n",
    "print(feature_names)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "\n",
    "def get_models():\n",
    "\tmodels = {'lr': LogisticRegression()}\n",
    "\tmodels['knn'] = KNeighborsClassifier()\n",
    "\tmodels['cart'] = DecisionTreeClassifier()\n",
    "\tmodels['svm'] = SVC()\n",
    "\tmodels['bayes'] = GaussianNB()\n",
    "\treturn models\n",
    " \n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "\treturn cross_val_score(\n",
    "\t    model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    " \n",
    "# define dataset\n",
    "X = train\n",
    "y = train_labels\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\tprint('model %s accuracy: %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "bayes = GaussianNB()\n",
    "model = bayes.fit(train, train_labels)\n",
    "preds = bayes.predict(test)\n",
    "\n",
    "print(f\"accuracy: {accuracy_score(test_labels, preds)}\")\n",
    "cf_matrix = confusion_matrix(test_labels, preds)\n",
    "# print(confusion_matrix)\n",
    "# turn confusion matrix into percentages\n",
    "df_cm = cf_matrix.astype('float') / cf_matrix.sum(axis=1)[:, np.newaxis] \n",
    "plt.figure()\n",
    "heatmap = sns.heatmap(df_cm, cmap=\"Blues\", annot=True)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Naive base Model Results in confusion matrix')\n",
    "plt.show()  \n",
    "\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the spark sessison\n",
    "\n",
    "print('Closing spark session')\n",
    "spk.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'Int64Index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix2019/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix2019/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bassemblix2019/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix2019/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mda\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bassemblix2019/homes/dlsteur/Git_repos/Programming3/Assignment6/assignment6.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask_ml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m OneHotEncoder\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/dask/dataframe/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m         DataFrame,\n\u001b[1;32m      4\u001b[0m         Series,\n\u001b[1;32m      5\u001b[0m         Index,\n\u001b[1;32m      6\u001b[0m         _Frame,\n\u001b[1;32m      7\u001b[0m         map_partitions,\n\u001b[1;32m      8\u001b[0m         repartition,\n\u001b[1;32m      9\u001b[0m         to_datetime,\n\u001b[1;32m     10\u001b[0m         to_timedelta,\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m \u001b[39mimport\u001b[39;00m Aggregation\n\u001b[1;32m     13\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m         from_array,\n\u001b[1;32m     15\u001b[0m         from_pandas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m         read_fwf,\n\u001b[1;32m     32\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/dask/dataframe/core.py:54\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdelayed\u001b[39;00m \u001b[39mimport\u001b[39;00m delayed, Delayed, unpack_collections\n\u001b[1;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhighlevelgraph\u001b[39;00m \u001b[39mimport\u001b[39;00m HighLevelGraph\n\u001b[0;32m---> 54\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m methods\n\u001b[1;32m     55\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maccessor\u001b[39;00m \u001b[39mimport\u001b[39;00m DatetimeAccessor, StringAccessor\n\u001b[1;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcategorical\u001b[39;00m \u001b[39mimport\u001b[39;00m CategoricalAccessor, categorize\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/dask/dataframe/methods.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m is_categorical_dtype, union_categoricals\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtlz\u001b[39;00m \u001b[39mimport\u001b[39;00m partition\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     is_series_like,\n\u001b[1;32m     10\u001b[0m     is_index_like,\n\u001b[1;32m     11\u001b[0m     is_dataframe_like,\n\u001b[1;32m     12\u001b[0m     hash_object_dispatch,\n\u001b[1;32m     13\u001b[0m     group_split_dispatch,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m Dispatch\n\u001b[1;32m     17\u001b[0m \u001b[39m# ---------------------------------\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# indexing\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# ---------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/dask/dataframe/utils.py:361\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[39mreturn\u001b[39;00m _nonempty_scalar(x)\n\u001b[1;32m    358\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDon\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt know how to create metadata from \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(x))\n\u001b[0;32m--> 361\u001b[0m _numeric_index_types \u001b[39m=\u001b[39m (pd\u001b[39m.\u001b[39;49mInt64Index, pd\u001b[39m.\u001b[39mFloat64Index, pd\u001b[39m.\u001b[39mUInt64Index)\n\u001b[1;32m    363\u001b[0m meta_nonempty \u001b[39m=\u001b[39m Dispatch(\u001b[39m\"\u001b[39m\u001b[39mmeta_nonempty\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    366\u001b[0m \u001b[39m@meta_nonempty\u001b[39m\u001b[39m.\u001b[39mregister(\u001b[39mobject\u001b[39m)\n\u001b[1;32m    367\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmeta_nonempty_object\u001b[39m(x):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'Int64Index'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask_ml.preprocessing import OneHotEncoder\n",
    "from dask_ml import model_selection\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from dask.distributed import Client\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "import joblib\n",
    "import time\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    " \n",
    "path='/data/dataprocessing/interproscan/all_bacilli.tsv'\n",
    "# path='/students/2021-2022/master/DaanSteur_DSLS/part.tsv' # 1million lines\n",
    "\n",
    "def read_clean(path):\n",
    "    \"\"\" Function to read the data and some data processing\n",
    "    Arguments:\n",
    "        path: path where the file is located as string\n",
    "       \n",
    "    Return:\n",
    "        ddf: dask dataframe\n",
    "    \"\"\"\n",
    "    ddf = dd.read_csv(path,sep='\\t',usecols=[0,2,6,7,11],header=None,blocksize=\"50MB\")\n",
    "\n",
    "    #Renaming the columns\n",
    "    ddf=ddf.rename(columns={0: \"Protein\", 2:'Sequence_length', 6:'Start', 7:'Stop', 11:'Interpro_accession',}) \n",
    "    \n",
    "    #deleting the null interpro accessions\n",
    "    ddf=ddf.loc[~(ddf['Interpro_accession']=='-')]\n",
    "\n",
    "    #dropping duplicates\n",
    "    ddf=ddf.drop_duplicates()\n",
    "\n",
    "    #creating feat length column\n",
    "    ddf['feat_len']=(ddf.Stop-ddf.Start)/ddf['Sequence_length']\n",
    "    return ddf\n",
    "\n",
    "def fun1(x):\n",
    "    \"\"\" Function to check whether an Interpro accession is large or small.Used with apply method\n",
    "    Arguments:\n",
    "        x: dask dataframe  \n",
    "    Return:\n",
    "        0: if Interpro accession is small\n",
    "        1: if Interpro accession is large\n",
    "    \"\"\"\n",
    "    return 1 if x['feat_len']>.90 else 0\n",
    "\n",
    "\n",
    "def find_size(x):\n",
    "    \"\"\" Function that returns proteins with atleast one small and large interpro accession.\n",
    "        Used with groupby apply method\n",
    "    Arguments:\n",
    "        x:groupby object grouped on protein\n",
    "    Return:\n",
    "        x:Returns the datframe\n",
    "    \"\"\"\n",
    "    m=np.mean(x['size'])\n",
    "    \n",
    "    if (m>0) and (m<1) : \n",
    "        return x\n",
    "\n",
    "def final_df(s_pivot,l_df):\n",
    "    \"\"\" Function to merge large dataframe with small dataframe on protein.\n",
    "    Arguments:\n",
    "        s_pivot: Pivoted dataframe in which columns are the name of small interpro accession \n",
    "                 and values are their counts\n",
    "        l_df:  Dask datframe containing the largest interpro accession of a protein\n",
    "    Return:\n",
    "        f_df: Final dataframe for machine learning\n",
    "    \"\"\"\n",
    "    f_df=s_pivot.merge(l_df,how='inner',left_on='Protein',right_on='Protein')\n",
    "    f_df=f_df.drop(columns=['feat_len'])\n",
    "    f_df=f_df.replace(np.nan, 0)\n",
    "    f_df=f_df.reset_index()\n",
    "    return f_df\n",
    "\n",
    "## Machine learning\n",
    "\n",
    "def ml_dfs(f_df):\n",
    "    \"\"\" Function to perform one hot encoding on class labels and to do train test splitting\n",
    "    Arguments:\n",
    "        f_df: Dask dataframe with the count of small interpro accession and \n",
    "              the name of largest interpro accession of each protein\n",
    "    Return:\n",
    "        X_train,y_train: Training dask arrays\n",
    "        X_test,y_test: Testing dask arrays \n",
    "    \"\"\"\n",
    "    y = OneHotEncoder().fit_transform(f_df[['Interpro_accession']])\n",
    "    X=f_df.iloc[:,2:-1].to_dask_array(lengths=True)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2,random_state=42,convert_mixed_types=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ddf=read_clean(path)\n",
    "\n",
    "    ddf['size']=ddf.apply(lambda x:fun1(x),axis=1)\n",
    "\n",
    "    #creating dataframe with proteins which have atleast one big and small interpro accession\n",
    "    full_df=ddf.groupby(['Protein',]).apply(find_size,meta={'Protein':'str','Sequence_length':'int64','Start':'int64',\n",
    "'Stop':'int64','Interpro_accession':'str','feat_len':'int64','size':'int64',}).reset_index(drop=True)\n",
    "    \n",
    "    #splittng dataframe into two containing large and small interpro accessions\n",
    "    large_df=full_df.loc[full_df['size']==1]\n",
    "    small_df=full_df.loc[full_df['size']==0]\n",
    "\n",
    "    client = Client()\n",
    "    with joblib.parallel_backend('dask'):\n",
    "\n",
    "        #Finding the largest interpro accession of a protein\n",
    "        l_df = large_df.loc[large_df.groupby(['Protein'])['feat_len'].transform(max) == large_df['feat_len'],['Protein','Interpro_accession','feat_len',]]\n",
    "        #counting the number of each small interpro accession#transform('count')#agg('count').reset_index()\n",
    "        s_df=small_df.groupby(['Protein','Interpro_accession'])['size'].agg('count').reset_index()\n",
    "\n",
    "        #catego\n",
    "        s_df=s_df.categorize(columns=['Interpro_accession'])\n",
    "        l_df=l_df.categorize(columns=['Interpro_accession'])\n",
    "\n",
    "        s_pivot=dd.reshape.pivot_table(s_df,index='Protein',columns='Interpro_accession', values='size')\n",
    "\n",
    "        f_df=final_df(s_pivot,l_df)\n",
    "\n",
    "        X_train, X_test, y_train, y_test=ml_dfs(f_df)\n",
    "\n",
    "    mid_time=(time.time() - st)/60\n",
    "    print('Entering ML part', mid_time, 'minutes')\n",
    "\n",
    "\n",
    "    with joblib.parallel_backend('dask'):\n",
    "        # creating randomforestclassifier object\n",
    "        clf = RandomForestClassifier(random_state=0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        # performing predictions on the test dataset\n",
    "        y_pred = clf.predict(X_test)\n",
    "        # using metrics module for accuracy calculation\n",
    "        accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        filename = '/students/2021-2022/master/programming3/randforest_model.pkl'\n",
    "        pickle.dump(clf, open(filename, 'wb'))\n",
    "        \n",
    "        X_df=dd.from_dask_array(X_train,columns=f_df.columns[2:-1])\n",
    "\n",
    "        X_df.to_csv('/students/2021-2022/master/DaanSteur_DSLS/X_train.csv')\n",
    "\n",
    "        print(\"ACCURACY OF THE MODEL: \", accuracy)\n",
    "        \n",
    "    # get the end time\n",
    "    et = time.time()\n",
    "\n",
    "    # get the execution time\n",
    "    elapsed_time = (et - st)/60\n",
    "    print('Execution time:', elapsed_time, 'minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "485f6e6640557d45f29956b0d574060939044db058f805100f7096fc920bb713"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
