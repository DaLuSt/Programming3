{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 7: XML Parsing with PySpark and key information extraction from research articles.\n",
    "In the ever-expanding realm of bioinformatics and biomedical research, extracting information from vast repositories of scientific literature is a crucial task. Assignment 7 is set on an exciting journey into the world of data science and natural language processing. The objective being to use PySpark to parse PubMed XML files and extract key information from research articles. \n",
    "\n",
    "### Introduction\n",
    "Scientific literature, especially in the domain of molecular biology and biochemistry, is a goldmine of knowledge. PubMed, as one of the largest repositories of biomedical literature, offers a treasure trove of research articles. However, making sense of this wealth of information can be daunting. This assignment addresses this challenge by developing a script capable of processing PubMed XML files and organizing the data into a PySpark dataframe.\n",
    "\n",
    "The key information that will be extracted includes:\n",
    "\n",
    "- PubMed ID\n",
    "- First Author\n",
    "- Last Author\n",
    "- Year published\n",
    "- Title\n",
    "- Journal Title\n",
    "- Length of Abstract (if Abstract text is present).\n",
    "- A column of references in a list variable, if references are present for the article.\n",
    "\n",
    "Furthermore, this assignment also involves the creation of a second dataframe to answer specific questions such as:\n",
    "\n",
    "- Number of articles per First Author\n",
    "- Number of articles per Year\n",
    "- Minimum, maximum, Average length of an abstract\n",
    "- Average Number of articles per Journal Title per Year\n",
    "\n",
    "### Deliverables\n",
    "To successfully complete this assignment, this script should be able to take one or more XML files as input and perform the following tasks:\n",
    "\n",
    "- Parse PubMed XML files into a PySpark dataframe.\n",
    "- Extract and organize the specified information from the articles.\n",
    "- Create a secondary dataframes to answer the provided questions.\n",
    "\n",
    "\n",
    "### Run code\n",
    "To execute the script, navigate to your terminal and use the following command:\n",
    "\n",
    "```\n",
    "python3 Assignment7.py [file_limit] (file_limit is optional)\n",
    "```\n",
    "\n",
    "### output\n",
    "Upon running the code, the script will generate multiple CSV files. The first CSV file will contain the parsed data from the PubMed XML files, while the remaining CSV files will contain the answers to the questions posed in this assignment. All output files will be located in an \"output\" folder, which the script will create for your convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Title: Replacement Assignment\n",
    "Author: Daan Steur\n",
    "Date: 05/10/2023\n",
    "Description: This script parses a PubMed XML file using PySpark and processes the data to obtain key information from research articles.\n",
    "Usage: python3 Assignment7.py [file_limit] (file_limit is optional)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "def create_spark_session(app_name):\n",
    "    \"\"\"\n",
    "    Creates a Spark session.\n",
    "\n",
    "    Args:\n",
    "        app_name (str): Name for the Spark application.\n",
    "\n",
    "    Returns:\n",
    "        SparkSession: A Spark session, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a Spark session with the provided app name\n",
    "        spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during Spark session creation\n",
    "        print(f\"An error occurred while creating the Spark session: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_articles_dataframe(spark, input_directory, file_limit=None):\n",
    "    \"\"\"\n",
    "    Parses PubMed XML files in the input directory and creates a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_directory (str): Path to the directory containing PubMed XML files.\n",
    "        file_limit (int): Limit for the number of files to process (optional).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A PySpark DataFrame containing article data, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define the schema for the articles DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"PubMedID\", StringType(), nullable=True),\n",
    "            StructField(\"FirstAuthor\", StringType(), nullable=True),\n",
    "            StructField(\"LastAuthor\", StringType(), nullable=True),\n",
    "            StructField(\"Year\", StringType(), nullable=True),\n",
    "            StructField(\"Title\", StringType(), nullable=True),\n",
    "            StructField(\"JournalTitle\", StringType(), nullable=True),\n",
    "            StructField(\"Abstract\", StringType(), nullable=True),\n",
    "            StructField(\"References\", StringType(), nullable=True)\n",
    "        ])\n",
    "\n",
    "        # Create an empty DataFrame for articles with the defined schema\n",
    "        articles_df = spark.createDataFrame([], schema=schema)\n",
    "\n",
    "        # Initialize a counter\n",
    "        file_count = 0\n",
    "\n",
    "        # Iterate over XML files in the input directory\n",
    "        for filename in os.listdir(input_directory):\n",
    "            if filename.endswith(\".xml\"):\n",
    "                filepath = os.path.join(input_directory, filename)\n",
    "                \n",
    "                # Parse the XML file\n",
    "                tree = ET.parse(filepath)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                # Extract data from XML and create rows\n",
    "                for article in root.findall(\".//PubmedArticle\"):\n",
    "                    pubmed_id = article.find(\".//PMID\").text\n",
    "                    first_author = article.find(\".//AuthorList/Author[1]/LastName\").text\n",
    "                    last_author = article.find(\".//AuthorList/Author[last()]/LastName\").text\n",
    "                    pub_year = article.find(\".//PubDate/Year\").text if article.find(\".//PubDate/Year\") is not None else \"Unknown\"\n",
    "                    title = article.find(\".//ArticleTitle\").text\n",
    "                    journal_title = article.find(\".//Journal/Title\").text\n",
    "                    abstract = article.find(\".//Abstract/AbstractText\")\n",
    "                    abstract_text = abstract.text if abstract is not None else \"\"\n",
    "                    \n",
    "                    # Extract references if available\n",
    "                    references = [ref.text for ref in article.findall(\".//PubmedData/ReferenceList/Reference/ArticleIdList/ArticleId[@IdType='pubmed']\")]\n",
    "\n",
    "                    # Append the data to the DataFrame\n",
    "                    articles_df = articles_df.union(\n",
    "                        spark.createDataFrame([(pubmed_id, first_author, last_author, pub_year, title, journal_title, abstract_text, references)], \n",
    "                                             schema=schema)\n",
    "                    )\n",
    "\n",
    "                    # Increment the file count\n",
    "                    file_count += 1\n",
    "\n",
    "                    # Check if the file limit has been reached\n",
    "                    if file_limit is not None and file_count >= file_limit:\n",
    "                        break\n",
    "                \n",
    "                # Check if the file limit has been reached\n",
    "                if file_limit is not None and file_count >= file_limit:\n",
    "                    break\n",
    "\n",
    "        return articles_df\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during DataFrame creation\n",
    "        print(f\"An error occurred while creating the DataFrame: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def save_dataframe_as_csv(dataframe, output_path):\n",
    "    \"\"\"\n",
    "    Saves a PySpark DataFrame as a CSV file.\n",
    "\n",
    "    Args:\n",
    "        dataframe (DataFrame): The DataFrame to be saved.\n",
    "        output_path (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataframe.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "        print(f\"DataFrame saved as CSV to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame as CSV: {str(e)}\")\n",
    "\n",
    "\n",
    "def create_and_save_analysis_dataframes(articles_df):\n",
    "    \"\"\"\n",
    "    Creates and saves analysis DataFrames based on the provided article DataFrame.\n",
    "\n",
    "    Args:\n",
    "        articles_df (DataFrame): The PySpark DataFrame containing article data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create the DataFrame to answer questions\n",
    "        author_counts = articles_df.groupBy(\"FirstAuthor\").count().alias(\"ArticleCountPerAuthor\")\n",
    "        year_counts = articles_df.groupBy(\"Year\").count().alias(\"ArticleCountPerYear\")\n",
    "        abstract_lengths = articles_df.groupBy().agg(avg(col(\"Abstract\").cast(\"string\").cast(IntegerType())).alias(\"AvgAbstractLength\"))\n",
    "        journal_year_counts = articles_df.groupBy(\"JournalTitle\", \"Year\").count().alias(\"ArticleCountPerJournalTitlePerYear\")\n",
    "\n",
    "        # Save the DataFrames to CSV files\n",
    "        save_dataframe_as_csv(author_counts, \"output/author_counts\")\n",
    "        save_dataframe_as_csv(year_counts, \"output/year_counts\")\n",
    "        save_dataframe_as_csv(abstract_lengths, \"output/abstract_lengths\")\n",
    "        save_dataframe_as_csv(journal_year_counts, \"output/journal_year_counts\")\n",
    "\n",
    "        print(\"Analysis DataFrames created and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating and saving analysis DataFrames: {str(e)}\")\n",
    "\n",
    "    \n",
    "def combine_csv_files(input_folder, output_file):\n",
    "    \"\"\"\n",
    "    Combines multiple CSV files into a single CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing CSV files to be combined.\n",
    "        output_file (str): Path to the output combined CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get a list of all CSV files in the input folder\n",
    "        csv_files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "        if not csv_files:\n",
    "            print(\"No CSV files found in the input folder.\")\n",
    "            return\n",
    "\n",
    "        # Read the first CSV file to get the header\n",
    "        first_csv = pd.read_csv(os.path.join(input_folder, csv_files[0]))\n",
    "        header = list(first_csv.columns)\n",
    "\n",
    "        # Create an empty DataFrame to store the combined data\n",
    "        combined_df = pd.DataFrame(columns=header)\n",
    "\n",
    "        # Append data from each CSV file to the combined DataFrame\n",
    "        for csv_file in csv_files:\n",
    "            csv_path = os.path.join(input_folder, csv_file)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "        # Save the combined DataFrame to the output CSV file\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined data saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining CSV files: {str(e)}\")\n",
    "\n",
    "    \n",
    "def delete_files_except_combined_csv(folder_path, combined_csv_filename):\n",
    "    \"\"\"\n",
    "    Deletes all files in a folder except the combined CSV file.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing files to be deleted.\n",
    "        combined_csv_filename (str): Name of the combined CSV file to be retained.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename != combined_csv_filename:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "        print(f\"Deleted all files except {combined_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while deleting files: {str(e)}\")\n",
    "\n",
    "\n",
    "def main(input_directory, file_limit=None):\n",
    "    \"\"\"\n",
    "    Main function to execute the PubMed XML data parsing and processing.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): Path to the directory containing PubMed XML files.\n",
    "        file_limit (int): Limit for the number of files to process (optional).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(\"output\"):\n",
    "            os.makedirs(\"output\")\n",
    "\n",
    "        # Initialize a Spark session\n",
    "        spark = create_spark_session(\"PubMedXMLParser\")\n",
    "\n",
    "        # Create articles DataFrame\n",
    "        articles_df = create_articles_dataframe(spark, input_directory, file_limit)\n",
    "\n",
    "        # Save the entire DataFrame to a single CSV file\n",
    "        save_dataframe_as_csv(articles_df, \"output/parsed_data\")\n",
    "\n",
    "        # Create and save analysis DataFrames\n",
    "        create_and_save_analysis_dataframes(articles_df)\n",
    "        \n",
    "        input_folder = \"output/parsed_data\"\n",
    "        output_file = \"output/parsed_data/combined_data.csv\"\n",
    "        combine_csv_files(input_folder, output_file)\n",
    "        \n",
    "        \n",
    "        combined_csv_filename = \"combined_data.csv\"\n",
    "        delete_files_except_combined_csv(input_folder, combined_csv_filename)\n",
    "\n",
    "        # Stop the Spark session\n",
    "        spark.stop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the main function: {str(e)}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"output/parsed_data\"\n",
    "    input_dir = \"/data/datasets/NCBI/PubMed/\"\n",
    "    \n",
    "    # Check if the file limit is provided as a command line argument\n",
    "    if len(sys.argv) > 1:\n",
    "        try:\n",
    "            file_limit = int(sys.argv[1])\n",
    "        except ValueError:\n",
    "            print(\"Invalid file limit. Please provide an integer.\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        # If no file limit is provided, set it to None to parse all files\n",
    "        file_limit = None\n",
    "\n",
    "    main(input_dir, file_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Title: Replacement Assignment XML parser\n",
    "Author: Daan Steur\n",
    "Date: 05/10/2023\n",
    "Description: This script parses a PubMed XML file using PySpark and processes the data to obtain key information from research articles.\n",
    "Usage: python3 Assignment7.py [file_limit] (file_limit is optional)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "def create_spark_session(app_name):\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the Spark session: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_articles_dataframe(spark, input_directory, file_limit=None):\n",
    "    try:\n",
    "        schema = StructType([\n",
    "            StructField(\"PubMedID\", StringType(), nullable=True),\n",
    "            StructField(\"FirstAuthor\", StringType(), nullable=True),\n",
    "            StructField(\"LastAuthor\", StringType(), nullable=True),\n",
    "            StructField(\"Year\", StringType(), nullable=True),\n",
    "            StructField(\"Title\", StringType(), nullable=True),\n",
    "            StructField(\"JournalTitle\", StringType(), nullable=True),\n",
    "            StructField(\"Abstract\", StringType(), nullable=True),\n",
    "            StructField(\"AbstractLength\", IntegerType(), nullable=True),\n",
    "            StructField(\"References\", StringType(), nullable=True)\n",
    "        ])\n",
    "\n",
    "        articles_df = spark.createDataFrame([], schema=schema)\n",
    "\n",
    "        file_count = 0\n",
    "\n",
    "        for filename in os.listdir(input_directory):\n",
    "            if filename.endswith(\".xml\"):\n",
    "                filepath = os.path.join(input_directory, filename)\n",
    "                \n",
    "                tree = ET.parse(filepath)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                article_data = []\n",
    "\n",
    "                for article in root.findall(\".//PubmedArticle\"):\n",
    "                    pubmed_id = article.find(\".//PMID\")\n",
    "                    first_author = article.find(\".//AuthorList/Author[1]/LastName\")\n",
    "                    last_author = article.find(\".//AuthorList/Author[last()]/LastName\")\n",
    "                    pub_year = article.find(\".//PubDate/Year\")\n",
    "                    title = article.find(\".//ArticleTitle\")\n",
    "                    journal_title = article.find(\".//Journal/Title\")\n",
    "                    abstract = article.find(\".//Abstract/AbstractText\")\n",
    "                    \n",
    "                    pubmed_id_text = pubmed_id.text if pubmed_id is not None else \"Unknown\"\n",
    "                    first_author_text = first_author.text if first_author is not None else \"Unknown\"\n",
    "                    last_author_text = last_author.text if last_author is not None else \"Unknown\"\n",
    "                    pub_year_text = pub_year.text if pub_year is not None else \"Unknown\"\n",
    "                    title_text = title.text if title is not None else \"Unknown\"\n",
    "                    journal_title_text = journal_title.text if journal_title is not None else \"Unknown\"\n",
    "                    abstract_text = abstract.text if abstract is not None else \"\"\n",
    "                    \n",
    "                    # Calculate abstract length\n",
    "                    abstract_length = len(abstract_text)\n",
    "\n",
    "                    references = [ref.text for ref in article.findall(\".//PubmedData/ReferenceList/Reference/ArticleIdList/ArticleId[@IdType='pubmed']\")]\n",
    "\n",
    "                    article_data.append((pubmed_id_text, first_author_text, last_author_text, pub_year_text, title_text, journal_title_text, abstract_text, abstract_length, references))\n",
    "\n",
    "                articles_df = articles_df.union(\n",
    "                    spark.createDataFrame(article_data, schema=schema)\n",
    "                )\n",
    "\n",
    "                file_count += 1\n",
    "\n",
    "                if file_limit is not None and file_count >= file_limit:\n",
    "                    break\n",
    "                \n",
    "        return articles_df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the DataFrame: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_dataframe_as_csv(dataframe, output_path):\n",
    "    try:\n",
    "        dataframe.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "        print(f\"DataFrame saved as CSV to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame as CSV: {str(e)}\")\n",
    "\n",
    "\n",
    "def create_and_save_questions_dataframes(articles_df):\n",
    "    try:\n",
    "        # Create the DataFrame to answer questions\n",
    "        author_counts = articles_df.groupBy(\"FirstAuthor\").count().alias(\"ArticleCountPerAuthor\")\n",
    "        year_counts = articles_df.groupBy(\"Year\").count().alias(\"ArticleCountPerYear\")\n",
    "        abstract_stats = articles_df.agg(\n",
    "            min(col(\"AbstractLength\")).alias(\"MinAbstractLength\"),\n",
    "            max(col(\"AbstractLength\")).alias(\"MaxAbstractLength\"),\n",
    "            avg(col(\"AbstractLength\")).alias(\"AvgAbstractLength\")\n",
    "        )\n",
    "        journal_year_counts = articles_df.groupBy(\"JournalTitle\", \"Year\").count().alias(\"ArticleCountPerJournalTitlePerYear\")\n",
    "\n",
    "        # Save the DataFrames to CSV files\n",
    "        save_dataframe_as_csv(author_counts, \"output/author_counts.csv\")\n",
    "        save_dataframe_as_csv(year_counts, \"output/year_counts.csv\")\n",
    "        save_dataframe_as_csv(abstract_stats, \"output/abstract_stats.csv\")\n",
    "        save_dataframe_as_csv(journal_year_counts, \"output/journal_year_counts.csv\")\n",
    "\n",
    "        print(\"Analysis DataFrames created and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating and saving analysis DataFrames: {str(e)}\")\n",
    "\n",
    "    \n",
    "def combine_and_delete_files(input_folder, output_file, combined_csv_filename):\n",
    "    \"\"\"\n",
    "    Combines multiple CSV files into a single CSV file and deletes all files in a folder except the combined CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing CSV files to be combined.\n",
    "        output_file (str): Path to the output combined CSV file.\n",
    "        combined_csv_filename (str): Name of the combined CSV file to be retained.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get a list of all CSV files in the input folder\n",
    "        csv_files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "        if not csv_files:\n",
    "            print(\"No CSV files found in the input folder.\")\n",
    "            return\n",
    "\n",
    "        # Initialize an empty DataFrame to store the combined data\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over each CSV file and concatenate them\n",
    "        for csv_file in csv_files:\n",
    "            csv_path = os.path.join(input_folder, csv_file)\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            except pd.errors.ParserError:\n",
    "                print(f\"Skipping file {csv_file} due to parsing error.\")\n",
    "\n",
    "        # Save the combined DataFrame to the output CSV file\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined data saved to {output_file}\")\n",
    "\n",
    "        # Delete all files in the folder except the combined CSV file\n",
    "        for filename in os.listdir(input_folder):\n",
    "            if filename != combined_csv_filename:\n",
    "                file_path = os.path.join(input_folder, filename)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "        print(f\"Deleted all files except {combined_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining and deleting files: {str(e)}\")\n",
    "\n",
    "def csv_name_change_logfile_delete(folder, new_name, confirm_logfile_delete=\"y\"):\n",
    "    \"\"\"\n",
    "    Rename CSV files in the specified folder to the new name and optionally delete other files.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The path to the folder containing CSV files.\n",
    "        new_name (str): The new name to assign to CSV files.\n",
    "        confirm_logfile_delete (str, optional): A confirmation prompt for deleting other files.\n",
    "            Defaults to \"y\". Set to \"n\" to skip deletion.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            old_path = os.path.join(folder, filename)\n",
    "            new_path = os.path.join(folder, new_name)\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Renamed {old_path} to {new_path}\")\n",
    "\n",
    "    # Ask the user for confirmation before deleting files\n",
    "    if confirm_logfile_delete == \"y\":\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            if os.path.isfile(file_path) and filename != new_name:\n",
    "                os.remove(file_path)\n",
    "        print(f\"Deleted all files except {new_name}\")\n",
    "    else:\n",
    "        print(\"Files were not deleted.\")\n",
    "\n",
    "\n",
    "def main(input_directory, file_limit = 1):\n",
    "    try:\n",
    "        # create output folder if it does not exist\n",
    "        if not os.path.exists(\"output\"):\n",
    "            os.makedirs(\"output\")\n",
    "\n",
    "        # Define folder paths and filenames\n",
    "        parsed_data_folder = \"output/parsed_data\"\n",
    "        output_file = os.path.join(parsed_data_folder, \"combined_data.csv\")\n",
    "        combined_csv_filename = \"combined_data.csv\"\n",
    "\n",
    "        # Initialize a Spark session\n",
    "        spark = create_spark_session(\"PubMedXMLParser\")\n",
    "\n",
    "        # Create articles DataFrame\n",
    "        articles_df = create_articles_dataframe(spark, input_directory, file_limit)\n",
    "\n",
    "        # Save the entire DataFrame to a single CSV file\n",
    "        save_dataframe_as_csv(articles_df, parsed_data_folder)\n",
    "        print(\"Parsed data saved to CSV files.\")\n",
    "\n",
    "        # Create and save analysis DataFrames\n",
    "        create_and_save_questions_dataframes(articles_df)\n",
    "        print(\"Analysis DataFrames created and saved.\")\n",
    "\n",
    "        # Combine output CSV files and delete all files except the combined CSV file\n",
    "        combine_and_delete_files(parsed_data_folder, output_file, combined_csv_filename)\n",
    "        print(f\"Combined data saved to {output_file}.\")\n",
    "        \n",
    "        # Rename the CSV files and delete all files except the renamed CSV files\n",
    "        csv_name_change_logfile_delete(\"output/abstract_lengths\", \"abstract_lengths.csv\", confirm_logfile_delete=\"y\")\n",
    "        csv_name_change_logfile_delete(\"output/author_counts\", \"author_counts.csv\", confirm_logfile_delete=\"y\")\n",
    "        csv_name_change_logfile_delete(\"output/journal_year_counts\", \"journal_year_counts.csv\", confirm_logfile_delete=\"y\")\n",
    "        csv_name_change_logfile_delete(\"output/year_counts\", \"year_counts.csv\", confirm_logfile_delete=\"y\")\n",
    "\n",
    "        # Stop the Spark session\n",
    "        spark.stop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the main function: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# input_dir = \"/data/datasets/NCBI/PubMed/\"\n",
    "\n",
    "    # # Check if the file limit is provided as a command line argument\n",
    "    # if len(sys.argv) > 1:\n",
    "    #     try:\n",
    "    #         file_limit = int(sys.argv[1])\n",
    "    #     except ValueError:\n",
    "    #         print(\"Invalid file limit. Please provide an integer.\")\n",
    "    #         sys.exit(1)\n",
    "    # else:\n",
    "    #     # If no file limit is provided, set it to None to parse all files\n",
    "    #     file_limit = None\n",
    "\n",
    "    # file_limit = 1\n",
    "main(\"/data/datasets/NCBI/PubMed/\", file_limit = 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
