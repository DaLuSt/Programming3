{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 7: XML Parsing with PySpark and key information extraction from research articles.\n",
    "In the ever-expanding realm of bioinformatics and biomedical research, extracting information from vast repositories of scientific literature is a crucial task. Assignment 7 is set on an exciting journey into the world of data science and natural language processing. The objective being to use PySpark to parse PubMed XML files and extract key information from research articles. \n",
    "\n",
    "### Introduction\n",
    "Scientific literature, especially in the domain of molecular biology and biochemistry, is a goldmine of knowledge. PubMed, as one of the largest repositories of biomedical literature, offers a treasure trove of research articles. However, making sense of this wealth of information can be daunting. This assignment addresses this challenge by developing a script capable of processing PubMed XML files and organizing the data into a PySpark dataframe.\n",
    "\n",
    "The key information that will be extracted includes:\n",
    "\n",
    "- PubMed ID\n",
    "- First Author\n",
    "- Last Author\n",
    "- Year published\n",
    "- Title\n",
    "- Journal Title\n",
    "- Length of Abstract (if Abstract text is present).\n",
    "- A column of references in a list variable, if references are present for the article.\n",
    "\n",
    "Furthermore, this assignment also involves the creation of a second dataframe to answer specific questions such as:\n",
    "\n",
    "- Number of articles per First Author\n",
    "- Number of articles per Year\n",
    "- Minimum, maximum, Average length of an abstract\n",
    "- Average Number of articles per Journal Title per Year\n",
    "\n",
    "### Deliverables\n",
    "To successfully complete this assignment, this script should be able to take one or more XML files as input and perform the following tasks:\n",
    "\n",
    "- Parse PubMed XML files into a PySpark dataframe.\n",
    "- Extract and organize the specified information from the articles.\n",
    "- Create a secondary dataframes to answer the provided questions.\n",
    "\n",
    "\n",
    "### Run code\n",
    "To execute the script, navigate to your terminal and use the following command:\n",
    "\n",
    "```\n",
    "python3 Assignment7.py [file_limit] (file_limit is optional / default is 1)\n",
    "```\n",
    "\n",
    "### output\n",
    "Upon running the code, the script will generate multiple CSV files. The first CSV file will contain the parsed data from the PubMed XML files, while the remaining CSV files will contain the answers to the questions posed in this assignment. All output files will be located in an \"output\" folder, which the script will create for your convenience.\n",
    "\n",
    "- parsed_data folder (combined_data.csv) : contains the parsed data from the PubMed XML files.\n",
    "- abstract_length folder (abstract_length.csv) : contains the answer to the question \"What is the minimum, maximum, and average length of an abstract?\" \n",
    "- author_count folder (author_count.csv)  : contains the answer to the question \"How many articles were published per author?\"\n",
    "- journal_year_count folder (journal_year_count.csv) : contains the answer to the question \"How many articles were published per journal per year?\" \n",
    "- year_count folder (year_count.csv) : contains the answer to the question \"How many articles were published per year?\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Title: Replacement Assignment XML parser\n",
    "Author: Daan Steur\n",
    "Date: 05/10/2023\n",
    "Description: This script parses a PubMed XML file using PySpark and processes the data to obtain key information from research articles.\n",
    "Usage: python3 Assignment7.py [file_limit] (file_limit is optional)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "def create_spark_session(app_name, num_executors=16, executor_cores=16, executor_memory='128g', driver_memory='128g'):\n",
    "    \"\"\"\n",
    "    Create a Spark session with specified configurations.\n",
    "\n",
    "    Args:\n",
    "        app_name (str): Name for the Spark application.\n",
    "        num_executors (int, optional): Number of executor instances (default is 16).\n",
    "        executor_cores (int, optional): Number of CPU cores per executor (default is 16).\n",
    "        executor_memory (str, optional): Memory per executor (e.g., '128g' for 128 gigabytes, default is '128g').\n",
    "        driver_memory (str, optional): Memory for the driver (e.g., '128g' for 128 gigabytes, default is '128g').\n",
    "\n",
    "    Returns:\n",
    "        SparkSession: A Spark session, or None if an error occurs during session creation.\n",
    "\n",
    "    Example:\n",
    "        spark = create_spark_session(\"PubMedXMLParser\", num_executors=16, executor_cores=16, executor_memory='128g', driver_memory='128g')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.executor.instances\", num_executors) \\\n",
    "            .config(\"spark.executor.cores\", executor_cores) \\\n",
    "            .config(\"spark.executor.memory\", executor_memory) \\\n",
    "            .config(\"spark.driver.memory\", driver_memory) \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the Spark session: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_articles_dataframe(spark, input_directory, file_limit=1):\n",
    "    \"\"\"\n",
    "    Parse PubMed XML files in the input directory and create a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_directory (str): Path to the directory containing PubMed XML files.\n",
    "        file_limit (int, optional): Limit for the number of files to process (default is None).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A PySpark DataFrame containing article data, or None if an error occurs.\n",
    "\n",
    "    Raises:\n",
    "        Exception: An error occurred during DataFrame creation.\n",
    "\n",
    "    Example:\n",
    "        spark = create_spark_session(\"PubMedXMLParser\")\n",
    "        articles_df = create_articles_dataframe(spark, \"/path/to/xml/files\", file_limit=10)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schema = StructType([\n",
    "            StructField(\"PubMedID\", StringType(), nullable=True),\n",
    "            StructField(\"FirstAuthor\", StringType(), nullable=True),\n",
    "            StructField(\"LastAuthor\", StringType(), nullable=True),\n",
    "            StructField(\"Year\", StringType(), nullable=True),\n",
    "            StructField(\"Title\", StringType(), nullable=True),\n",
    "            StructField(\"JournalTitle\", StringType(), nullable=True),\n",
    "            StructField(\"Abstract\", StringType(), nullable=True),\n",
    "            StructField(\"AbstractLength\", IntegerType(), nullable=True),\n",
    "            StructField(\"References\", StringType(), nullable=True)\n",
    "        ])\n",
    "\n",
    "        articles_df = spark.createDataFrame([], schema=schema)\n",
    "\n",
    "        file_count = 0\n",
    "\n",
    "        for filename in os.listdir(input_directory):\n",
    "            if filename.endswith(\".xml\"):\n",
    "                filepath = os.path.join(input_directory, filename)\n",
    "                \n",
    "                tree = ET.parse(filepath)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                article_data = []\n",
    "\n",
    "                for article in root.findall(\".//PubmedArticle\"):\n",
    "                    pubmed_id = article.find(\".//PMID\")\n",
    "                    first_author = article.find(\".//AuthorList/Author[1]/LastName\")\n",
    "                    last_author = article.find(\".//AuthorList/Author[last()]/LastName\")\n",
    "                    pub_year = article.find(\".//PubDate/Year\")\n",
    "                    title = article.find(\".//ArticleTitle\")\n",
    "                    journal_title = article.find(\".//Journal/Title\")\n",
    "                    abstract = article.find(\".//Abstract/AbstractText\")\n",
    "                    \n",
    "                    pubmed_id_text = pubmed_id.text if pubmed_id is not None else \"Unknown\"\n",
    "                    first_author_text = first_author.text if first_author is not None else \"Unknown\"\n",
    "                    last_author_text = last_author.text if last_author is not None else \"Unknown\"\n",
    "                    pub_year_text = pub_year.text if pub_year is not None else \"Unknown\"\n",
    "                    title_text = title.text if title is not None else \"Unknown\"\n",
    "                    journal_title_text = journal_title.text if journal_title is not None else \"Unknown\"\n",
    "                    abstract_text = abstract.text if abstract is not None else \"\"\n",
    "                    \n",
    "                    # Calculate abstract length\n",
    "                    abstract_length = len(abstract_text)\n",
    "\n",
    "                    references = [ref.text for ref in article.findall(\".//PubmedData/ReferenceList/Reference/ArticleIdList/ArticleId[@IdType='pubmed']\")]\n",
    "\n",
    "                    article_data.append((pubmed_id_text, first_author_text, last_author_text, pub_year_text, title_text, journal_title_text, abstract_text, abstract_length, references))\n",
    "\n",
    "                articles_df = articles_df.union(\n",
    "                    spark.createDataFrame(article_data, schema=schema)\n",
    "                )\n",
    "\n",
    "                file_count += 1\n",
    "\n",
    "                if file_limit is not None and file_count >= file_limit:\n",
    "                    break\n",
    "                \n",
    "        return articles_df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the DataFrame: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def save_dataframe_as_csv(dataframe, output_path):\n",
    "    try:\n",
    "        dataframe.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "        print(f\"DataFrame saved as CSV to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame as CSV: {str(e)}\")\n",
    "        \n",
    "        \n",
    "def create_and_save_analysis_dataframes(articles_df):\n",
    "    \"\"\"\n",
    "    Create and save analysis DataFrames based on the provided article DataFrame.\n",
    "\n",
    "    This function performs various analyses on the input DataFrame and saves the results as CSV files.\n",
    "\n",
    "    Args:\n",
    "        articles_df (DataFrame): The PySpark DataFrame containing article data.\n",
    "\n",
    "    Raises:\n",
    "        Exception: An error occurred during DataFrame creation or saving.\n",
    "\n",
    "    Example:\n",
    "        spark = create_spark_session(\"PubMedXMLParser\")\n",
    "        articles_df = create_articles_dataframe(spark, \"/path/to/xml/files\", file_limit=10)\n",
    "        create_and_save_analysis_dataframes(articles_df)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate author counts\n",
    "        author_counts = articles_df.groupBy(\"FirstAuthor\").count().alias(\"ArticleCountPerAuthor\")\n",
    "\n",
    "        # Calculate year-wise article counts\n",
    "        year_counts = articles_df.groupBy(\"Year\").count().alias(\"ArticleCountPerYear\")\n",
    "        \n",
    "        # Calculate min, max, and average of AbstractLength column\n",
    "        abstract_lengths = articles_df.agg(\n",
    "            min(col(\"AbstractLength\")).alias(\"MinAbstractLength\"),\n",
    "            max(col(\"AbstractLength\")).alias(\"MaxAbstractLength\"),\n",
    "            avg(col(\"AbstractLength\")).alias(\"AvgAbstractLength\")\n",
    "        )\n",
    "        \n",
    "        # Calculate journal and year-wise article counts\n",
    "        journal_year_counts = articles_df.groupBy(\"JournalTitle\", \"Year\").count().alias(\"ArticleCountPerJournalTitlePerYear\")\n",
    "\n",
    "        # Save the DataFrames to CSV files\n",
    "        save_dataframe_as_csv(author_counts, \"output/author_counts\")\n",
    "        save_dataframe_as_csv(year_counts, \"output/year_counts\")\n",
    "        save_dataframe_as_csv(abstract_lengths, \"output/abstract_lengths\")\n",
    "        save_dataframe_as_csv(journal_year_counts, \"output/journal_year_counts\")\n",
    "\n",
    "        print(\"Analysis DataFrames created and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating and saving analysis DataFrames: {str(e)}\")\n",
    "\n",
    "\n",
    "    \n",
    "def combine_and_delete_files(input_folder, output_file, combined_csv_filename):\n",
    "    \"\"\"\n",
    "    Combines multiple CSV files into a single CSV file and deletes all files in a folder except the combined CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing CSV files to be combined.\n",
    "        output_file (str): Path to the output combined CSV file.\n",
    "        combined_csv_filename (str): Name of the combined CSV file to be retained.\n",
    "\n",
    "    Raises:\n",
    "        Exception: An error occurred during file processing.\n",
    "\n",
    "    Example:\n",
    "        >>> combine_and_delete_files(\"/path/to/csv/files\", \"output/combined_data.csv\", \"combined_data.csv\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get a list of all CSV files in the input folder\n",
    "        csv_files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "        if not csv_files:\n",
    "            print(\"No CSV files found in the input folder.\")\n",
    "            return\n",
    "\n",
    "        # Initialize an empty DataFrame to store the combined data\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        # Iterate over each CSV file and concatenate them\n",
    "        for csv_file in csv_files:\n",
    "            csv_path = os.path.join(input_folder, csv_file)\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            except pd.errors.ParserError:\n",
    "                print(f\"Skipping file {csv_file} due to parsing error.\")\n",
    "\n",
    "        # Save the combined DataFrame to the output CSV file\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined data saved to {output_file}\")\n",
    "\n",
    "        # Delete all files in the folder except the combined CSV file\n",
    "        for filename in os.listdir(input_folder):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            if os.path.isfile(file_path) and filename != combined_csv_filename:\n",
    "                os.remove(file_path)\n",
    "        print(f\"Deleted all files except {combined_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error combining and deleting files: {str(e)}\")\n",
    "        \n",
    "\n",
    "def csv_name_change_logfile_delete(folder, new_name, confirm_logfile_delete=\"y\"):\n",
    "    \"\"\"\n",
    "    Rename CSV files in the specified folder to the new name and optionally delete other files.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The path to the folder containing CSV files.\n",
    "        new_name (str): The new name to assign to CSV files.\n",
    "        confirm_logfile_delete (str, optional): A confirmation prompt for deleting other files.\n",
    "            Defaults to \"y\". Set to \"n\" to skip deletion.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Invalid confirmation input.\n",
    "\n",
    "    Example:\n",
    "        csv_name_change_logfile_delete(\"/path/to/csv/files\", \"new_name.csv\", \"y\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for filename in os.listdir(folder):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                old_path = os.path.join(folder, filename)\n",
    "                new_path = os.path.join(folder, new_name)\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\"Renamed {old_path} to {new_path}\")\n",
    "\n",
    "        # Ask the user for confirmation before deleting files\n",
    "        if confirm_logfile_delete == \"y\":\n",
    "            for filename in os.listdir(folder):\n",
    "                file_path = os.path.join(folder, filename)\n",
    "                if os.path.isfile(file_path) and filename != new_name:\n",
    "                    os.remove(file_path)\n",
    "            print(f\"Deleted all files except {new_name}\")\n",
    "        elif confirm_logfile_delete == \"n\":\n",
    "            print(\"Files were not deleted.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid confirmation input. Use 'y' to confirm deletion or 'n' to skip.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error renaming and deleting files: {str(e)}\")\n",
    "\n",
    "\n",
    "def main(input_directory, file_limit = 1):\n",
    "    try:\n",
    "        # create output folder if it does not exist\n",
    "        if not os.path.exists(\"output\"):\n",
    "            os.makedirs(\"output\")\n",
    "\n",
    "        # Define folder paths and filenames\n",
    "        parsed_data_folder = \"output/parsed_data\"\n",
    "        output_file = os.path.join(parsed_data_folder, \"combined_data.csv\")\n",
    "        combined_csv_filename = \"combined_data.csv\"\n",
    "\n",
    "        # Initialize a Spark session\n",
    "        spark = create_spark_session(\"PubMedXMLParser\")\n",
    "\n",
    "        # Create articles DataFrame\n",
    "        articles_df = create_articles_dataframe(spark, input_directory, file_limit)\n",
    "\n",
    "        # Save the entire DataFrame to a single CSV file\n",
    "        save_dataframe_as_csv(articles_df, parsed_data_folder)\n",
    "\n",
    "        # Create and save analysis DataFrames\n",
    "        create_and_save_analysis_dataframes(articles_df)\n",
    "\n",
    "        # Combine output CSV files and delete all files except the combined CSV file\n",
    "        combine_and_delete_files(parsed_data_folder, output_file, combined_csv_filename)\n",
    "        print(f\"Combined data saved to {output_file}.\")\n",
    "        \n",
    "        # Rename the CSV files and delete all files except the renamed CSV files\n",
    "        csv_name_change_logfile_delete(\"output/abstract_lengths\", \"abstract_lengths.csv\", confirm_logfile_delete=\"y\")\n",
    "        csv_name_change_logfile_delete(\"output/author_counts\", \"author_counts.csv\", confirm_logfile_delete=\"y\")\n",
    "        csv_name_change_logfile_delete(\"output/journal_year_counts\", \"journal_year_counts.csv\", confirm_logfile_delete=\"y\")\n",
    "        csv_name_change_logfile_delete(\"output/year_counts\", \"year_counts.csv\", confirm_logfile_delete=\"y\")\n",
    "\n",
    "        # Stop the Spark session\n",
    "        spark.stop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the main function: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Check if the file limit is provided as a command line argument\n",
    "    if len(sys.argv) > 1:\n",
    "        try:\n",
    "            file_limit = int(sys.argv[1])\n",
    "        except ValueError:\n",
    "            print(\"Invalid file limit. Please provide an integer.\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        # If no file limit is provided, set it to None to parse all files\n",
    "        file_limit = None\n",
    "\n",
    "    main(\"/data/datasets/NCBI/PubMed/\", file_limit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
