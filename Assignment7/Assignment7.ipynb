{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 7: XML Parsing with PySpark and key information extraction from research articles.\n",
    "In the ever-expanding realm of bioinformatics and biomedical research, extracting information from vast repositories of scientific literature is a crucial task. Assignment 7 is set on an exciting journey into the world of data science and natural language processing. The objective being to use PySpark to parse PubMed XML files and extract key information from research articles. \n",
    "\n",
    "### Introduction\n",
    "Scientific literature, especially in the domain of molecular biology and biochemistry, is a goldmine of knowledge. PubMed, as one of the largest repositories of biomedical literature, offers a treasure trove of research articles. However, making sense of this wealth of information can be daunting. This assignment addresses this challenge by developing a script capable of processing PubMed XML files and organizing the data into a PySpark dataframe.\n",
    "\n",
    "The key information that will be extracted includes:\n",
    "\n",
    "- PubMed ID\n",
    "- First Author\n",
    "- Last Author\n",
    "- Year published\n",
    "- Title\n",
    "- Journal Title\n",
    "- Length of Abstract (if Abstract text is present).\n",
    "- A column of references in a list variable, if references are present for the article.\n",
    "\n",
    "Furthermore, this assignment also involves the creation of a second dataframe to answer specific questions such as:\n",
    "\n",
    "- Number of articles per First Author\n",
    "- Number of articles per Year\n",
    "- Minimum, maximum, Average length of an abstract\n",
    "- Average Number of articles per Journal Title per Year\n",
    "\n",
    "### Deliverables\n",
    "To successfully complete this assignment, this script should be able to take one or more XML files as input and perform the following tasks:\n",
    "\n",
    "- Parse PubMed XML files into a PySpark dataframe.\n",
    "- Extract and organize the specified information from the articles.\n",
    "- Create a secondary dataframe to answer the provided questions.\n",
    "\n",
    "\n",
    "### Run code\n",
    "To execute the script, navigate to your terminal and use the following command:\n",
    "\n",
    "```\n",
    "python3 Assignment7.py\n",
    "```\n",
    "\n",
    "### output\n",
    "Upon running the code, the script will generate two CSV files. The first CSV file will contain the parsed data from the PubMed XML files, while the second CSV file will contain the answers to the questions posed in this assignment. Both output files will be located in an \"output\" folder, which the script will create for your convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to output/parsed_data.csv/combined_data.csv\n",
      "Deleted all files except combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "def create_spark_session(app_name):\n",
    "    return SparkSession.builder.appName(app_name).getOrCreate()\n",
    "\n",
    "def create_articles_dataframe(spark, input_directory, file_limit=None):\n",
    "    # Define the schema for the articles DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"PubMedID\", StringType(), nullable=True),\n",
    "        StructField(\"FirstAuthor\", StringType(), nullable=True),\n",
    "        StructField(\"LastAuthor\", StringType(), nullable=True),\n",
    "        StructField(\"Year\", StringType(), nullable=True),\n",
    "        StructField(\"Title\", StringType(), nullable=True),\n",
    "        StructField(\"JournalTitle\", StringType(), nullable=True),\n",
    "        StructField(\"Abstract\", StringType(), nullable=True),\n",
    "        StructField(\"References\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    # Create an empty DataFrame for articles with the defined schema\n",
    "    articles_df = spark.createDataFrame([], schema=schema)\n",
    "\n",
    "    # Initialize a counter\n",
    "    file_count = 0\n",
    "\n",
    "    # Iterate over XML files in the input directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            filepath = os.path.join(input_directory, filename)\n",
    "            \n",
    "            # Parse the XML file\n",
    "            tree = ET.parse(filepath)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Extract data from XML and create rows\n",
    "            for article in root.findall(\".//PubmedArticle\"):\n",
    "                pubmed_id = article.find(\".//PMID\").text\n",
    "                first_author = article.find(\".//AuthorList/Author[1]/LastName\").text\n",
    "                last_author = article.find(\".//AuthorList/Author[last()]/LastName\").text\n",
    "                pub_year = article.find(\".//PubDate/Year\").text if article.find(\".//PubDate/Year\") is not None else \"Unknown\"\n",
    "                title = article.find(\".//ArticleTitle\").text\n",
    "                journal_title = article.find(\".//Journal/Title\").text\n",
    "                abstract = article.find(\".//Abstract/AbstractText\")\n",
    "                abstract_text = abstract.text if abstract is not None else \"\"\n",
    "                \n",
    "                # Extract references if available\n",
    "                references = [ref.text for ref in article.findall(\".//PubmedData/ReferenceList/Reference/ArticleIdList/ArticleId[@IdType='pubmed']\")]\n",
    "\n",
    "                # Append the data to the DataFrame\n",
    "                articles_df = articles_df.union(\n",
    "                    spark.createDataFrame([(pubmed_id, first_author, last_author, pub_year, title, journal_title, abstract_text, references)], \n",
    "                                         schema=schema)\n",
    "                )\n",
    "\n",
    "                # Increment the file count\n",
    "                file_count += 1\n",
    "\n",
    "                # Check if the file limit has been reached\n",
    "                if file_limit is not None and file_count >= file_limit:\n",
    "                    break\n",
    "            \n",
    "            # Check if the file limit has been reached\n",
    "            if file_limit is not None and file_count >= file_limit:\n",
    "                break\n",
    "\n",
    "    return articles_df\n",
    "\n",
    "def save_dataframe_as_csv(dataframe, output_path):\n",
    "    dataframe.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "\n",
    "def create_and_save_analysis_dataframes(articles_df):\n",
    "    # Create a second DataFrame to answer questions\n",
    "    author_counts = articles_df.groupBy(\"FirstAuthor\").count().alias(\"ArticleCountPerAuthor\")\n",
    "    year_counts = articles_df.groupBy(\"Year\").count().alias(\"ArticleCountPerYear\")\n",
    "    abstract_lengths = articles_df.groupBy().agg(avg(col(\"Abstract\").cast(\"string\").cast(IntegerType())).alias(\"AvgAbstractLength\"))\n",
    "    journal_year_counts = articles_df.groupBy(\"JournalTitle\", \"Year\").count().alias(\"ArticleCountPerJournalTitlePerYear\")\n",
    "\n",
    "    # Save the second DataFrames to CSV files\n",
    "    save_dataframe_as_csv(author_counts, \"output/author_counts.csv\")\n",
    "    save_dataframe_as_csv(year_counts, \"output/year_counts.csv\")\n",
    "    save_dataframe_as_csv(abstract_lengths, \"output/abstract_lengths.csv\")\n",
    "    save_dataframe_as_csv(journal_year_counts, \"output/journal_year_counts.csv\")\n",
    "    \n",
    "    \n",
    "def combine_csv_files(input_folder, output_file):\n",
    "    # Get a list of all CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found in the input folder.\")\n",
    "        return\n",
    "\n",
    "    # Read the first CSV file to get the header\n",
    "    first_csv = pd.read_csv(os.path.join(input_folder, csv_files[0]))\n",
    "    header = list(first_csv.columns)\n",
    "\n",
    "    # Create an empty DataFrame to store the combined data\n",
    "    combined_df = pd.DataFrame(columns=header)\n",
    "\n",
    "    # Append data from each CSV file to the combined DataFrame\n",
    "    for csv_file in csv_files:\n",
    "        csv_path = os.path.join(input_folder, csv_file)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to the output CSV file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined data saved to {output_file}\")\n",
    "    \n",
    "def delete_files_except_combined_csv(folder_path, combined_csv_filename):\n",
    "    try:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename != combined_csv_filename:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "        print(f\"Deleted all files except {combined_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while deleting files: {str(e)}\")\n",
    "\n",
    "\n",
    "def main(input_directory, file_limit=None):\n",
    "    # Initialize a Spark session\n",
    "    spark = create_spark_session(\"PubMedParser\")\n",
    "\n",
    "    # Create articles DataFrame\n",
    "    articles_df = create_articles_dataframe(spark, input_directory, file_limit)\n",
    "\n",
    "    # Save the entire DataFrame to a single CSV file\n",
    "    save_dataframe_as_csv(articles_df, \"output/parsed_data.csv\")\n",
    "\n",
    "    # Create and save analysis DataFrames\n",
    "    create_and_save_analysis_dataframes(articles_df)\n",
    "    \n",
    "    input_folder = \"output/parsed_data.csv\"\n",
    "    output_file = \"output/parsed_data.csv/combined_data.csv\"\n",
    "    combine_csv_files(input_folder, output_file)\n",
    "    \n",
    "    combined_csv_filename = \"combined_data.csv\"\n",
    "    delete_files_except_combined_csv(input_folder, combined_csv_filename)\n",
    "\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "input_dir = \"/data/datasets/NCBI/PubMed/\"\n",
    "# Set this to the desired file limit, or None to parse all files\n",
    "# file_limit = sys.argv[1] if len(sys.argv) > 1 else None\n",
    "file_limit = 5\n",
    "\n",
    "main(input_dir, file_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
