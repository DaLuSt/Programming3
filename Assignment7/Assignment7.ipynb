{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 7: XML Parsing with PySpark and key information extraction from research articles.\n",
    "In the ever-expanding realm of bioinformatics and biomedical research, extracting information from vast repositories of scientific literature is a crucial task. Assignment 7 is set on an exciting journey into the world of data science and natural language processing. The objective being to use PySpark to parse PubMed XML files and extract key information from research articles. \n",
    "\n",
    "### Introduction\n",
    "Scientific literature, especially in the domain of molecular biology and biochemistry, is a goldmine of knowledge. PubMed, as one of the largest repositories of biomedical literature, offers a treasure trove of research articles. However, making sense of this wealth of information can be daunting. This assignment addresses this challenge by developing a script capable of processing PubMed XML files and organizing the data into a PySpark dataframe.\n",
    "\n",
    "The key information that will be extracted includes:\n",
    "\n",
    "- PubMed ID\n",
    "- First Author\n",
    "- Last Author\n",
    "- Year published\n",
    "- Title\n",
    "- Journal Title\n",
    "- Length of Abstract (if Abstract text is present).\n",
    "- A column of references in a list variable, if references are present for the article.\n",
    "\n",
    "Furthermore, this assignment also involves the creation of a second dataframe to answer specific questions such as:\n",
    "\n",
    "- Number of articles per First Author\n",
    "- Number of articles per Year\n",
    "- Minimum, maximum, Average length of an abstract\n",
    "- Average Number of articles per Journal Title per Year\n",
    "\n",
    "### Deliverables\n",
    "To successfully complete this assignment, this script should be able to take one or more XML files as input and perform the following tasks:\n",
    "\n",
    "- Parse PubMed XML files into a PySpark dataframe.\n",
    "- Extract and organize the specified information from the articles.\n",
    "- Create a secondary dataframes to answer the provided questions.\n",
    "\n",
    "\n",
    "### Run code\n",
    "To execute the script, navigate to your terminal and use the following command:\n",
    "\n",
    "```\n",
    "python3 Assignment7.py [file_limit] (file_limit is optional)\n",
    "```\n",
    "\n",
    "### output\n",
    "Upon running the code, the script will generate multiple CSV files. The first CSV file will contain the parsed data from the PubMed XML files, while the remaining CSV files will contain the answers to the questions posed in this assignment. All output files will be located in an \"output\" folder, which the script will create for your convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to output/parsed_data.csv/combined_data.csv\n",
      "Deleted all files except combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Title: Replacement Assignment\n",
    "Author: Daan Steur\n",
    "Date: 05/10/2023\n",
    "Description: This script parses a PubMed XML file using PySpark and processes the data to obtain key information from research articles.\n",
    "Usage: python3 Assignment7.py [file_limit] (file_limit is optional)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "def create_spark_session(app_name):\n",
    "    \"\"\"\n",
    "    Creates a Spark session.\n",
    "\n",
    "    Args:\n",
    "        app_name (str): Name for the Spark application.\n",
    "\n",
    "    Returns:\n",
    "        SparkSession: A Spark session, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a Spark session with the provided app name\n",
    "        spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during Spark session creation\n",
    "        print(f\"An error occurred while creating the Spark session: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_articles_dataframe(spark, input_directory, file_limit=None):\n",
    "    \"\"\"\n",
    "    Parses PubMed XML files in the input directory and creates a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session.\n",
    "        input_directory (str): Path to the directory containing PubMed XML files.\n",
    "        file_limit (int): Limit for the number of files to process (optional).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A PySpark DataFrame containing article data, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define the schema for the articles DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"PubMedID\", StringType(), nullable=True),\n",
    "            StructField(\"FirstAuthor\", StringType(), nullable=True),\n",
    "            StructField(\"LastAuthor\", StringType(), nullable=True),\n",
    "            StructField(\"Year\", StringType(), nullable=True),\n",
    "            StructField(\"Title\", StringType(), nullable=True),\n",
    "            StructField(\"JournalTitle\", StringType(), nullable=True),\n",
    "            StructField(\"Abstract\", StringType(), nullable=True),\n",
    "            StructField(\"References\", StringType(), nullable=True)\n",
    "        ])\n",
    "\n",
    "        # Create an empty DataFrame for articles with the defined schema\n",
    "        articles_df = spark.createDataFrame([], schema=schema)\n",
    "\n",
    "        # Initialize a counter\n",
    "        file_count = 0\n",
    "\n",
    "        # Iterate over XML files in the input directory\n",
    "        for filename in os.listdir(input_directory):\n",
    "            if filename.endswith(\".xml\"):\n",
    "                filepath = os.path.join(input_directory, filename)\n",
    "                \n",
    "                # Parse the XML file\n",
    "                tree = ET.parse(filepath)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                # Extract data from XML and create rows\n",
    "                for article in root.findall(\".//PubmedArticle\"):\n",
    "                    pubmed_id = article.find(\".//PMID\").text\n",
    "                    first_author = article.find(\".//AuthorList/Author[1]/LastName\").text\n",
    "                    last_author = article.find(\".//AuthorList/Author[last()]/LastName\").text\n",
    "                    pub_year = article.find(\".//PubDate/Year\").text if article.find(\".//PubDate/Year\") is not None else \"Unknown\"\n",
    "                    title = article.find(\".//ArticleTitle\").text\n",
    "                    journal_title = article.find(\".//Journal/Title\").text\n",
    "                    abstract = article.find(\".//Abstract/AbstractText\")\n",
    "                    abstract_text = abstract.text if abstract is not None else \"\"\n",
    "                    \n",
    "                    # Extract references if available\n",
    "                    references = [ref.text for ref in article.findall(\".//PubmedData/ReferenceList/Reference/ArticleIdList/ArticleId[@IdType='pubmed']\")]\n",
    "\n",
    "                    # Append the data to the DataFrame\n",
    "                    articles_df = articles_df.union(\n",
    "                        spark.createDataFrame([(pubmed_id, first_author, last_author, pub_year, title, journal_title, abstract_text, references)], \n",
    "                                             schema=schema)\n",
    "                    )\n",
    "\n",
    "                    # Increment the file count\n",
    "                    file_count += 1\n",
    "\n",
    "                    # Check if the file limit has been reached\n",
    "                    if file_limit is not None and file_count >= file_limit:\n",
    "                        break\n",
    "                \n",
    "                # Check if the file limit has been reached\n",
    "                if file_limit is not None and file_count >= file_limit:\n",
    "                    break\n",
    "\n",
    "        return articles_df\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions that may occur during DataFrame creation\n",
    "        print(f\"An error occurred while creating the DataFrame: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def save_dataframe_as_csv(dataframe, output_path):\n",
    "    \"\"\"\n",
    "    Saves a PySpark DataFrame as a CSV file.\n",
    "\n",
    "    Args:\n",
    "        dataframe (DataFrame): The DataFrame to be saved.\n",
    "        output_path (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataframe.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "        print(f\"DataFrame saved as CSV to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame as CSV: {str(e)}\")\n",
    "\n",
    "\n",
    "def create_and_save_analysis_dataframes(articles_df):\n",
    "    \"\"\"\n",
    "    Creates and saves analysis DataFrames based on the provided article DataFrame.\n",
    "\n",
    "    Args:\n",
    "        articles_df (DataFrame): The PySpark DataFrame containing article data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create the DataFrame to answer questions\n",
    "        author_counts = articles_df.groupBy(\"FirstAuthor\").count().alias(\"ArticleCountPerAuthor\")\n",
    "        year_counts = articles_df.groupBy(\"Year\").count().alias(\"ArticleCountPerYear\")\n",
    "        abstract_lengths = articles_df.groupBy().agg(avg(col(\"Abstract\").cast(\"string\").cast(IntegerType())).alias(\"AvgAbstractLength\"))\n",
    "        journal_year_counts = articles_df.groupBy(\"JournalTitle\", \"Year\").count().alias(\"ArticleCountPerJournalTitlePerYear\")\n",
    "\n",
    "        # Save the DataFrames to CSV files\n",
    "        save_dataframe_as_csv(author_counts, \"output/author_counts\")\n",
    "        save_dataframe_as_csv(year_counts, \"output/year_counts\")\n",
    "        save_dataframe_as_csv(abstract_lengths, \"output/abstract_lengths\")\n",
    "        save_dataframe_as_csv(journal_year_counts, \"output/journal_year_counts\")\n",
    "\n",
    "        print(\"Analysis DataFrames created and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating and saving analysis DataFrames: {str(e)}\")\n",
    "\n",
    "    \n",
    "def combine_csv_files(input_folder, output_file):\n",
    "    \"\"\"\n",
    "    Combines multiple CSV files into a single CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Path to the folder containing CSV files to be combined.\n",
    "        output_file (str): Path to the output combined CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get a list of all CSV files in the input folder\n",
    "        csv_files = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "        if not csv_files:\n",
    "            print(\"No CSV files found in the input folder.\")\n",
    "            return\n",
    "\n",
    "        # Read the first CSV file to get the header\n",
    "        first_csv = pd.read_csv(os.path.join(input_folder, csv_files[0]))\n",
    "        header = list(first_csv.columns)\n",
    "\n",
    "        # Create an empty DataFrame to store the combined data\n",
    "        combined_df = pd.DataFrame(columns=header)\n",
    "\n",
    "        # Append data from each CSV file to the combined DataFrame\n",
    "        for csv_file in csv_files:\n",
    "            csv_path = os.path.join(input_folder, csv_file)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "        # Save the combined DataFrame to the output CSV file\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined data saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining CSV files: {str(e)}\")\n",
    "\n",
    "    \n",
    "def delete_files_except_combined_csv(folder_path, combined_csv_filename):\n",
    "    \"\"\"\n",
    "    Deletes all files in a folder except the combined CSV file.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing files to be deleted.\n",
    "        combined_csv_filename (str): Name of the combined CSV file to be retained.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename != combined_csv_filename:\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.remove(file_path)\n",
    "        print(f\"Deleted all files except {combined_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while deleting files: {str(e)}\")\n",
    "\n",
    "\n",
    "def main(input_directory, file_limit=None):\n",
    "    \"\"\"\n",
    "    Main function to execute the PubMed XML data parsing and processing.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): Path to the directory containing PubMed XML files.\n",
    "        file_limit (int): Limit for the number of files to process (optional).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize a Spark session\n",
    "        spark = create_spark_session(\"PubMedXMLParser\")\n",
    "\n",
    "        # Create articles DataFrame\n",
    "        articles_df = create_articles_dataframe(spark, input_directory, file_limit)\n",
    "\n",
    "        # Save the entire DataFrame to a single CSV file\n",
    "        save_dataframe_as_csv(articles_df, \"output/parsed_data\")\n",
    "\n",
    "        # Create and save analysis DataFrames\n",
    "        create_and_save_analysis_dataframes(articles_df)\n",
    "        \n",
    "        input_folder = \"output/parsed_data\"\n",
    "        output_file = \"output/parsed_data/combined_data.csv\"\n",
    "        combine_csv_files(input_folder, output_file)\n",
    "        \n",
    "        \n",
    "        combined_csv_filename = \"combined_data.csv\"\n",
    "        delete_files_except_combined_csv(input_folder, combined_csv_filename)\n",
    "\n",
    "        # Stop the Spark session\n",
    "        spark.stop()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the main function: {str(e)}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"output/parsed_data\"\n",
    "    input_dir = \"/data/datasets/NCBI/PubMed/\"\n",
    "    \n",
    "    # Check if the file limit is provided as a command line argument\n",
    "    if len(sys.argv) > 1:\n",
    "        try:\n",
    "            file_limit = int(sys.argv[1])\n",
    "        except ValueError:\n",
    "            print(\"Invalid file limit. Please provide an integer.\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        # If no file limit is provided, set it to None to parse all files\n",
    "        file_limit = None\n",
    "\n",
    "    main(input_dir, file_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PubMedID</th>\n",
       "      <th>FirstAuthor</th>\n",
       "      <th>LastAuthor</th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>JournalTitle</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>References</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Moroi</td>\n",
       "      <td>Sato</td>\n",
       "      <td>1975</td>\n",
       "      <td>Comparison between procaine and isocarboxazid ...</td>\n",
       "      <td>Biochemical pharmacology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[11385576, 12851870, 15157825, 15480983, 15537...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>Chow</td>\n",
       "      <td>Mukerji</td>\n",
       "      <td>1975</td>\n",
       "      <td>Studies of oxygen binding energy to hemoglobin...</td>\n",
       "      <td>Biochemical and biophysical research communica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Makar</td>\n",
       "      <td>Tephly</td>\n",
       "      <td>1975</td>\n",
       "      <td>Formate assay in body fluids: application in m...</td>\n",
       "      <td>Biochemical medicine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Bryant</td>\n",
       "      <td>1975</td>\n",
       "      <td>Metal substitutions incarbonic anhydrase: a ha...</td>\n",
       "      <td>Biochemical and biophysical research communica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Hendrickson</td>\n",
       "      <td>Ward</td>\n",
       "      <td>1975</td>\n",
       "      <td>Atomic models for the polypeptide backbones of...</td>\n",
       "      <td>Biochemical and biophysical research communica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PubMedID  FirstAuthor LastAuthor  Year  \\\n",
       "0         8        Moroi       Sato  1975   \n",
       "1         6         Chow    Mukerji  1975   \n",
       "2         1        Makar     Tephly  1975   \n",
       "3         3        Smith     Bryant  1975   \n",
       "4         5  Hendrickson       Ward  1975   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Comparison between procaine and isocarboxazid ...   \n",
       "1  Studies of oxygen binding energy to hemoglobin...   \n",
       "2  Formate assay in body fluids: application in m...   \n",
       "3  Metal substitutions incarbonic anhydrase: a ha...   \n",
       "4  Atomic models for the polypeptide backbones of...   \n",
       "\n",
       "                                        JournalTitle  Abstract  \\\n",
       "0                           Biochemical pharmacology       NaN   \n",
       "1  Biochemical and biophysical research communica...       NaN   \n",
       "2                               Biochemical medicine       NaN   \n",
       "3  Biochemical and biophysical research communica...       NaN   \n",
       "4  Biochemical and biophysical research communica...       NaN   \n",
       "\n",
       "                                          References  \n",
       "0  [11385576, 12851870, 15157825, 15480983, 15537...  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"output/parsed_data/combined_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_abstract_lengths = \"output/author_counts\"\n",
    "new_name = \"author_counts.csv\"\n",
    "\n",
    "def csv_name_change_logfile_delete(folder, new_name, confirm_file_delete=\"y\"):\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            old_path = os.path.join(folder, filename)\n",
    "            new_path = os.path.join(folder, new_name)\n",
    "            os.rename(old_path, new_path)\n",
    "\n",
    "    # Ask the user for confirmation before deleting files\n",
    "    if confirm_file_delete == \"y\":\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            if os.path.isfile(file_path) and filename != new_name:\n",
    "                os.remove(file_path)\n",
    "    else:\n",
    "        print(\"Files were not deleted.\")\n",
    "\n",
    "csv_name_change_logfile_delete(folder_abstract_lengths, new_name, confirm_file_delete=\"y\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
